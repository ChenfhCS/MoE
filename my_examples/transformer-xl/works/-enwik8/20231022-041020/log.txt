====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-041020
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 508.81 | loss  4.12 | bpc   5.94979
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 522.73 | loss  3.46 | bpc   4.99064
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 521.56 | loss  3.25 | bpc   4.68298
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 524.69 | loss  3.13 | bpc   4.51172
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 526.24 | loss  3.01 | bpc   4.34366
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 532.94 | loss  3.06 | bpc   4.41361
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 529.99 | loss  2.92 | bpc   4.20776
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 528.55 | loss  2.88 | bpc   4.15793
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 527.91 | loss  2.83 | bpc   4.08976
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 533.86 | loss  2.74 | bpc   3.95927 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 529.10 | loss  2.92 | bpc   4.20856
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 532.22 | loss  2.87 | bpc   4.14180
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 528.83 | loss  3.06 | bpc   4.41887
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 526.66 | loss  2.87 | bpc   4.13782
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 525.75 | loss  2.76 | bpc   3.98613
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 527.56 | loss  2.75 | bpc   3.97457
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 528.91 | loss  2.62 | bpc   3.77639
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 533.06 | loss  2.83 | bpc   4.08786
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 525.89 | loss  2.58 | bpc   3.71611
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 526.81 | loss  2.55 | bpc   3.68407 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 530.38 | loss  2.71 | bpc   3.90555
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 527.94 | loss  2.95 | bpc   4.25864
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 529.69 | loss  2.76 | bpc   3.98812
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 531.38 | loss  2.56 | bpc   3.69594
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 530.24 | loss  2.53 | bpc   3.64619
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 531.29 | loss  2.66 | bpc   3.83703
| epoch   1 step      270 |    270 batches | lr 0.00025 | ms/batch 526.67 | loss  2.47 | bpc   3.56668
| epoch   1 step      280 |    280 batches | lr 0.00025 | ms/batch 526.29 | loss  2.56 | bpc   3.69591
| epoch   1 step      290 |    290 batches | lr 0.00025 | ms/batch 525.49 | loss  2.03 | bpc   2.92870
| epoch   1 step      300 |    300 batches | lr 0.00025 | ms/batch 533.22 | loss  1.79 | bpc   2.58424 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79]
| epoch   1 step      310 |    310 batches | lr 0.00025 | ms/batch 528.46 | loss  1.70 | bpc   2.45874
| epoch   1 step      320 |    320 batches | lr 0.00025 | ms/batch 526.40 | loss  1.94 | bpc   2.80490
| epoch   1 step      330 |    330 batches | lr 0.00025 | ms/batch 526.69 | loss  2.46 | bpc   3.55029
| epoch   1 step      340 |    340 batches | lr 0.00025 | ms/batch 529.85 | loss  2.30 | bpc   3.31539
| epoch   1 step      350 |    350 batches | lr 0.00025 | ms/batch 528.06 | loss  2.30 | bpc   3.31180
| epoch   1 step      360 |    360 batches | lr 0.00025 | ms/batch 529.35 | loss  2.30 | bpc   3.32053
| epoch   1 step      370 |    370 batches | lr 0.00025 | ms/batch 532.28 | loss  2.45 | bpc   3.53386
| epoch   1 step      380 |    380 batches | lr 0.00025 | ms/batch 530.80 | loss  2.36 | bpc   3.39894
| epoch   1 step      390 |    390 batches | lr 0.00025 | ms/batch 527.87 | loss  2.56 | bpc   3.69670
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 529.27 | loss  2.36 | bpc   3.41029 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36]
| epoch   1 step      410 |    410 batches | lr 0.00025 | ms/batch 529.49 | loss  2.26 | bpc   3.26191
| epoch   1 step      420 |    420 batches | lr 0.00025 | ms/batch 528.84 | loss  2.13 | bpc   3.07579
| epoch   1 step      430 |    430 batches | lr 0.00025 | ms/batch 528.18 | loss  2.44 | bpc   3.52631
| epoch   1 step      440 |    440 batches | lr 0.00025 | ms/batch 528.72 | loss  2.21 | bpc   3.18517
| epoch   1 step      450 |    450 batches | lr 0.00025 | ms/batch 528.77 | loss  2.32 | bpc   3.34118
| epoch   1 step      460 |    460 batches | lr 0.00025 | ms/batch 527.43 | loss  2.32 | bpc   3.34743
| epoch   1 step      470 |    470 batches | lr 0.00025 | ms/batch 531.89 | loss  2.21 | bpc   3.19205
| epoch   1 step      480 |    480 batches | lr 0.00025 | ms/batch 532.83 | loss  2.33 | bpc   3.35505
| epoch   1 step      490 |    490 batches | lr 0.00025 | ms/batch 529.75 | loss  2.16 | bpc   3.11582
| epoch   1 step      500 |    500 batches | lr 0.00025 | ms/batch 526.01 | loss  2.11 | bpc   3.03773 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11]
| epoch   1 step      510 |    510 batches | lr 0.00025 | ms/batch 526.48 | loss  2.16 | bpc   3.11929
| epoch   1 step      520 |    520 batches | lr 0.00025 | ms/batch 527.45 | loss  2.15 | bpc   3.09700
| epoch   1 step      530 |    530 batches | lr 0.00025 | ms/batch 528.17 | loss  2.08 | bpc   3.00509
| epoch   1 step      540 |    540 batches | lr 0.00025 | ms/batch 530.01 | loss  2.12 | bpc   3.05233
| epoch   1 step      550 |    550 batches | lr 0.00025 | ms/batch 531.21 | loss  2.22 | bpc   3.20382
| epoch   1 step      560 |    560 batches | lr 0.00025 | ms/batch 527.56 | loss  2.08 | bpc   3.00251
| epoch   1 step      570 |    570 batches | lr 0.00025 | ms/batch 527.29 | loss  2.15 | bpc   3.09481
| epoch   1 step      580 |    580 batches | lr 0.00025 | ms/batch 529.42 | loss  2.23 | bpc   3.21047
| epoch   1 step      590 |    590 batches | lr 0.00025 | ms/batch 528.46 | loss  2.14 | bpc   3.09141
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 527.84 | loss  2.14 | bpc   3.08616 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14]
| epoch   1 step      610 |    610 batches | lr 0.00025 | ms/batch 527.65 | loss  2.00 | bpc   2.88736
| epoch   1 step      620 |    620 batches | lr 0.00025 | ms/batch 527.85 | loss  1.98 | bpc   2.85889
| epoch   1 step      630 |    630 batches | lr 0.00025 | ms/batch 527.59 | loss  2.00 | bpc   2.89185
| epoch   1 step      640 |    640 batches | lr 0.00025 | ms/batch 527.26 | loss  2.05 | bpc   2.95878
| epoch   1 step      650 |    650 batches | lr 0.00025 | ms/batch 526.85 | loss  2.14 | bpc   3.08052
| epoch   1 step      660 |    660 batches | lr 0.00025 | ms/batch 527.03 | loss  2.23 | bpc   3.21822
| epoch   1 step      670 |    670 batches | lr 0.00025 | ms/batch 526.59 | loss  2.19 | bpc   3.15456
| epoch   1 step      680 |    680 batches | lr 0.00025 | ms/batch 526.87 | loss  1.90 | bpc   2.74500
| epoch   1 step      690 |    690 batches | lr 0.00025 | ms/batch 526.79 | loss  1.65 | bpc   2.37884
| epoch   1 step      700 |    700 batches | lr 0.00025 | ms/batch 527.03 | loss  2.21 | bpc   3.18187 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21]
| epoch   1 step      710 |    710 batches | lr 0.00025 | ms/batch 527.05 | loss  1.83 | bpc   2.63823
| epoch   1 step      720 |    720 batches | lr 0.00025 | ms/batch 526.93 | loss  2.03 | bpc   2.92602
| epoch   1 step      730 |    730 batches | lr 0.00025 | ms/batch 526.83 | loss  2.10 | bpc   3.03519
| epoch   1 step      740 |    740 batches | lr 0.00025 | ms/batch 527.79 | loss  1.91 | bpc   2.75432
| epoch   1 step      750 |    750 batches | lr 0.00025 | ms/batch 527.78 | loss  1.97 | bpc   2.84611
| epoch   1 step      760 |    760 batches | lr 0.00025 | ms/batch 526.80 | loss  1.88 | bpc   2.70725
| epoch   1 step      770 |    770 batches | lr 0.00025 | ms/batch 527.75 | loss  1.96 | bpc   2.83106
| epoch   1 step      780 |    780 batches | lr 0.00025 | ms/batch 527.60 | loss  2.11 | bpc   3.04028
| epoch   1 step      790 |    790 batches | lr 0.00025 | ms/batch 527.38 | loss  2.05 | bpc   2.95360
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 527.32 | loss  1.99 | bpc   2.87451 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99]
| epoch   1 step      810 |    810 batches | lr 0.00025 | ms/batch 528.00 | loss  1.97 | bpc   2.84093
| epoch   1 step      820 |    820 batches | lr 0.00025 | ms/batch 530.35 | loss  2.01 | bpc   2.89490
| epoch   1 step      830 |    830 batches | lr 0.00025 | ms/batch 524.91 | loss  1.84 | bpc   2.65341
| epoch   1 step      840 |    840 batches | lr 0.00025 | ms/batch 516.46 | loss  1.99 | bpc   2.86556
| epoch   1 step      850 |    850 batches | lr 0.00025 | ms/batch 514.25 | loss  2.01 | bpc   2.90127
| epoch   1 step      860 |    860 batches | lr 0.00025 | ms/batch 514.81 | loss  1.93 | bpc   2.78413
| epoch   1 step      870 |    870 batches | lr 0.00025 | ms/batch 515.30 | loss  1.90 | bpc   2.74150
| epoch   1 step      880 |    880 batches | lr 0.00025 | ms/batch 515.07 | loss  1.89 | bpc   2.73120
| epoch   1 step      890 |    890 batches | lr 0.00025 | ms/batch 514.84 | loss  1.79 | bpc   2.58429
| epoch   1 step      900 |    900 batches | lr 0.00025 | ms/batch 517.05 | loss  2.09 | bpc   3.02129 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09]
| epoch   1 step      910 |    910 batches | lr 0.00025 | ms/batch 516.16 | loss  1.81 | bpc   2.61685
| epoch   1 step      920 |    920 batches | lr 0.00025 | ms/batch 516.70 | loss  1.92 | bpc   2.77003
| epoch   1 step      930 |    930 batches | lr 0.00025 | ms/batch 520.54 | loss  1.82 | bpc   2.62759
| epoch   1 step      940 |    940 batches | lr 0.00025 | ms/batch 520.61 | loss  1.81 | bpc   2.61049
| epoch   1 step      950 |    950 batches | lr 0.00025 | ms/batch 521.15 | loss  1.88 | bpc   2.71353
| epoch   1 step      960 |    960 batches | lr 0.00025 | ms/batch 515.88 | loss  2.08 | bpc   2.99519
| epoch   1 step      970 |    970 batches | lr 0.00025 | ms/batch 518.50 | loss  1.89 | bpc   2.72556
| epoch   1 step      980 |    980 batches | lr 0.00025 | ms/batch 517.78 | loss  1.75 | bpc   2.53029
| epoch   1 step      990 |    990 batches | lr 0.00025 | ms/batch 517.15 | loss  1.88 | bpc   2.71697
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 515.40 | loss  1.82 | bpc   2.61946 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82]
| epoch   1 step     1010 |   1010 batches | lr 0.00025 | ms/batch 518.20 | loss  1.72 | bpc   2.48176
| epoch   1 step     1020 |   1020 batches | lr 0.00025 | ms/batch 514.43 | loss  1.92 | bpc   2.76333
| epoch   1 step     1030 |   1030 batches | lr 0.00025 | ms/batch 518.86 | loss  1.80 | bpc   2.59528
| epoch   1 step     1040 |   1040 batches | lr 0.00025 | ms/batch 514.81 | loss  1.73 | bpc   2.50046
| epoch   1 step     1050 |   1050 batches | lr 0.00025 | ms/batch 514.90 | loss  1.84 | bpc   2.64756
| epoch   1 step     1060 |   1060 batches | lr 0.00025 | ms/batch 519.11 | loss  2.04 | bpc   2.93914
| epoch   1 step     1070 |   1070 batches | lr 0.00025 | ms/batch 517.06 | loss  1.87 | bpc   2.69254
| epoch   1 step     1080 |   1080 batches | lr 0.00025 | ms/batch 515.82 | loss  1.74 | bpc   2.51480
| epoch   1 step     1090 |   1090 batches | lr 0.00025 | ms/batch 515.21 | loss  1.69 | bpc   2.44365
| epoch   1 step     1100 |   1100 batches | lr 0.00025 | ms/batch 515.31 | loss  1.80 | bpc   2.59684 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8]
| epoch   1 step     1110 |   1110 batches | lr 0.00025 | ms/batch 516.99 | loss  1.73 | bpc   2.49058
| epoch   1 step     1120 |   1120 batches | lr 0.00025 | ms/batch 514.47 | loss  1.75 | bpc   2.52618
| epoch   1 step     1130 |   1130 batches | lr 0.00025 | ms/batch 514.37 | loss  1.93 | bpc   2.77828
| epoch   1 step     1140 |   1140 batches | lr 0.00025 | ms/batch 515.19 | loss  1.71 | bpc   2.47322
| epoch   1 step     1150 |   1150 batches | lr 0.00025 | ms/batch 517.98 | loss  1.73 | bpc   2.50054
| epoch   1 step     1160 |   1160 batches | lr 0.00025 | ms/batch 513.59 | loss  1.69 | bpc   2.44352
| epoch   1 step     1170 |   1170 batches | lr 0.00025 | ms/batch 516.27 | loss  1.79 | bpc   2.58362
| epoch   1 step     1180 |   1180 batches | lr 0.00025 | ms/batch 514.89 | loss  1.76 | bpc   2.53556
| epoch   1 step     1190 |   1190 batches | lr 0.00025 | ms/batch 515.07 | loss  1.76 | bpc   2.53982
| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 516.64 | loss  1.85 | bpc   2.67238 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8, 1.73, 1.75, 1.93, 1.71, 1.73, 1.69, 1.79, 1.76, 1.76, 1.85]
| epoch   1 step     1210 |   1210 batches | lr 0.00025 | ms/batch 516.20 | loss  1.70 | bpc   2.45407
| epoch   1 step     1220 |   1220 batches | lr 0.00025 | ms/batch 515.72 | loss  1.75 | bpc   2.52065
| epoch   1 step     1230 |   1230 batches | lr 0.00025 | ms/batch 518.24 | loss  1.79 | bpc   2.58110
| epoch   1 step     1240 |   1240 batches | lr 0.00025 | ms/batch 516.56 | loss  1.60 | bpc   2.31149
| epoch   1 step     1250 |   1250 batches | lr 0.00025 | ms/batch 516.51 | loss  1.86 | bpc   2.68374
| epoch   1 step     1260 |   1260 batches | lr 0.00025 | ms/batch 515.75 | loss  1.82 | bpc   2.62233
| epoch   1 step     1270 |   1270 batches | lr 0.00025 | ms/batch 517.04 | loss  1.95 | bpc   2.80797
| epoch   1 step     1280 |   1280 batches | lr 0.00025 | ms/batch 514.34 | loss  1.74 | bpc   2.50443
| epoch   1 step     1290 |   1290 batches | lr 0.00025 | ms/batch 516.78 | loss  1.81 | bpc   2.60652
| epoch   1 step     1300 |   1300 batches | lr 0.00025 | ms/batch 519.91 | loss  1.87 | bpc   2.69376 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8, 1.73, 1.75, 1.93, 1.71, 1.73, 1.69, 1.79, 1.76, 1.76, 1.85, 1.7, 1.75, 1.79, 1.6, 1.86, 1.82, 1.95, 1.74, 1.81, 1.87]
| epoch   1 step     1310 |   1310 batches | lr 0.00025 | ms/batch 514.76 | loss  1.85 | bpc   2.66584
| epoch   1 step     1320 |   1320 batches | lr 0.00025 | ms/batch 517.67 | loss  1.81 | bpc   2.61413
| epoch   1 step     1330 |   1330 batches | lr 0.00025 | ms/batch 513.97 | loss  1.76 | bpc   2.53936
| epoch   1 step     1340 |   1340 batches | lr 0.00025 | ms/batch 515.36 | loss  1.84 | bpc   2.64830
| epoch   1 step     1350 |   1350 batches | lr 0.00025 | ms/batch 514.89 | loss  1.89 | bpc   2.72990
| epoch   1 step     1360 |   1360 batches | lr 0.00025 | ms/batch 517.41 | loss  1.89 | bpc   2.72095
| epoch   1 step     1370 |   1370 batches | lr 0.00025 | ms/batch 516.18 | loss  1.80 | bpc   2.59359
| epoch   1 step     1380 |   1380 batches | lr 0.00025 | ms/batch 517.86 | loss  1.67 | bpc   2.41218
| epoch   1 step     1390 |   1390 batches | lr 0.00025 | ms/batch 515.37 | loss  1.65 | bpc   2.37913
| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 513.52 | loss  1.64 | bpc   2.36774 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8, 1.73, 1.75, 1.93, 1.71, 1.73, 1.69, 1.79, 1.76, 1.76, 1.85, 1.7, 1.75, 1.79, 1.6, 1.86, 1.82, 1.95, 1.74, 1.81, 1.87, 1.85, 1.81, 1.76, 1.84, 1.89, 1.89, 1.8, 1.67, 1.65, 1.64]
| epoch   1 step     1410 |   1410 batches | lr 0.00025 | ms/batch 517.17 | loss  1.94 | bpc   2.80413
| epoch   1 step     1420 |   1420 batches | lr 0.00025 | ms/batch 512.27 | loss  1.87 | bpc   2.70382
| epoch   1 step     1430 |   1430 batches | lr 0.00025 | ms/batch 517.71 | loss  1.82 | bpc   2.63014
| epoch   1 step     1440 |   1440 batches | lr 0.00025 | ms/batch 515.99 | loss  2.01 | bpc   2.90169
| epoch   1 step     1450 |   1450 batches | lr 0.00025 | ms/batch 516.49 | loss  1.79 | bpc   2.58596
| epoch   1 step     1460 |   1460 batches | lr 0.00025 | ms/batch 514.76 | loss  1.80 | bpc   2.59557
| epoch   1 step     1470 |   1470 batches | lr 0.00025 | ms/batch 512.13 | loss  1.80 | bpc   2.59691
| epoch   1 step     1480 |   1480 batches | lr 0.00025 | ms/batch 513.89 | loss  1.73 | bpc   2.49933
| epoch   1 step     1490 |   1490 batches | lr 0.00025 | ms/batch 515.02 | loss  1.83 | bpc   2.63706
| epoch   1 step     1500 |   1500 batches | lr 0.00025 | ms/batch 512.14 | loss  1.90 | bpc   2.73524 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8, 1.73, 1.75, 1.93, 1.71, 1.73, 1.69, 1.79, 1.76, 1.76, 1.85, 1.7, 1.75, 1.79, 1.6, 1.86, 1.82, 1.95, 1.74, 1.81, 1.87, 1.85, 1.81, 1.76, 1.84, 1.89, 1.89, 1.8, 1.67, 1.65, 1.64, 1.94, 1.87, 1.82, 2.01, 1.79, 1.8, 1.8, 1.73, 1.83, 1.9]
| epoch   1 step     1510 |   1510 batches | lr 0.00025 | ms/batch 514.54 | loss  1.78 | bpc   2.57089
| epoch   1 step     1520 |   1520 batches | lr 0.00025 | ms/batch 514.24 | loss  1.87 | bpc   2.69394
| epoch   1 step     1530 |   1530 batches | lr 0.00025 | ms/batch 515.69 | loss  1.72 | bpc   2.47638
| epoch   1 step     1540 |   1540 batches | lr 0.00025 | ms/batch 516.95 | loss  1.72 | bpc   2.47471
| epoch   1 step     1550 |   1550 batches | lr 0.00025 | ms/batch 518.42 | loss  1.91 | bpc   2.74903
| epoch   1 step     1560 |   1560 batches | lr 0.00025 | ms/batch 514.40 | loss  1.78 | bpc   2.57393
| epoch   1 step     1570 |   1570 batches | lr 0.00025 | ms/batch 514.33 | loss  1.68 | bpc   2.41669
| epoch   1 step     1580 |   1580 batches | lr 0.00025 | ms/batch 516.11 | loss  1.86 | bpc   2.67639
| epoch   1 step     1590 |   1590 batches | lr 0.00025 | ms/batch 516.26 | loss  1.93 | bpc   2.78130
| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 517.80 | loss  1.91 | bpc   2.75594 | current losses [4.12, 3.46, 3.25, 3.13, 3.01, 3.06, 2.92, 2.88, 2.83, 2.74, 2.92, 2.87, 3.06, 2.87, 2.76, 2.75, 2.62, 2.83, 2.58, 2.55, 2.71, 2.95, 2.76, 2.56, 2.53, 2.66, 2.47, 2.56, 2.03, 1.79, 1.7, 1.94, 2.46, 2.3, 2.3, 2.3, 2.45, 2.36, 2.56, 2.36, 2.26, 2.13, 2.44, 2.21, 2.32, 2.32, 2.21, 2.33, 2.16, 2.11, 2.16, 2.15, 2.08, 2.12, 2.22, 2.08, 2.15, 2.23, 2.14, 2.14, 2.0, 1.98, 2.0, 2.05, 2.14, 2.23, 2.19, 1.9, 1.65, 2.21, 1.83, 2.03, 2.1, 1.91, 1.97, 1.88, 1.96, 2.11, 2.05, 1.99, 1.97, 2.01, 1.84, 1.99, 2.01, 1.93, 1.9, 1.89, 1.79, 2.09, 1.81, 1.92, 1.82, 1.81, 1.88, 2.08, 1.89, 1.75, 1.88, 1.82, 1.72, 1.92, 1.8, 1.73, 1.84, 2.04, 1.87, 1.74, 1.69, 1.8, 1.73, 1.75, 1.93, 1.71, 1.73, 1.69, 1.79, 1.76, 1.76, 1.85, 1.7, 1.75, 1.79, 1.6, 1.86, 1.82, 1.95, 1.74, 1.81, 1.87, 1.85, 1.81, 1.76, 1.84, 1.89, 1.89, 1.8, 1.67, 1.65, 1.64, 1.94, 1.87, 1.82, 2.01, 1.79, 1.8, 1.8, 1.73, 1.83, 1.9, 1.78, 1.87, 1.72, 1.72, 1.91, 1.78, 1.68, 1.86, 1.93, 1.91]
| epoch   1 step     1610 |   1610 batches | lr 0.00025 | ms/batch 516.07 | loss  1.95 | bpc   2.81533
| epoch   1 step     1620 |   1620 batches | lr 0.00025 | ms/batch 513.89 | loss  1.96 | bpc   2.82576
| epoch   1 step     1630 |   1630 batches | lr 0.00025 | ms/batch 513.09 | loss  1.84 | bpc   2.65049
| epoch   1 step     1640 |   1640 batches | lr 0.00025 | ms/batch 512.59 | loss  1.85 | bpc   2.66526
| epoch   1 step     1650 |   1650 batches | lr 0.00025 | ms/batch 512.73 | loss  1.72 | bpc   2.48310
