====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-014906
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 504.29 | loss  4.13 | bpc   5.95540
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 516.98 | loss  3.26 | bpc   4.70324
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 518.89 | loss  2.83 | bpc   4.07578
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 518.57 | loss  2.76 | bpc   3.98371
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 520.51 | loss  2.77 | bpc   3.99199
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 520.60 | loss  2.91 | bpc   4.20102
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 522.49 | loss  2.82 | bpc   4.06867
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 523.98 | loss  2.79 | bpc   4.03025
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 524.02 | loss  2.77 | bpc   3.99004
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 525.55 | loss  2.68 | bpc   3.86864 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 526.47 | loss  2.84 | bpc   4.09300
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 527.05 | loss  2.83 | bpc   4.08333
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 528.90 | loss  2.98 | bpc   4.29288
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 529.55 | loss  2.85 | bpc   4.11573
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 529.71 | loss  2.73 | bpc   3.94429
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 531.08 | loss  2.70 | bpc   3.89582
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 532.37 | loss  2.52 | bpc   3.63890
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 532.78 | loss  2.77 | bpc   4.00238
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 534.92 | loss  2.48 | bpc   3.58306
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 534.75 | loss  2.45 | bpc   3.53642 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 535.45 | loss  2.65 | bpc   3.82978
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 536.12 | loss  2.90 | bpc   4.17774
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 537.25 | loss  2.71 | bpc   3.90664
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 538.36 | loss  2.49 | bpc   3.59173
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 538.46 | loss  2.48 | bpc   3.57745
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 539.70 | loss  2.64 | bpc   3.80615
| epoch   1 step      270 |    270 batches | lr 0.00025 | ms/batch 540.19 | loss  2.45 | bpc   3.53364
| epoch   1 step      280 |    280 batches | lr 0.00025 | ms/batch 539.77 | loss  2.59 | bpc   3.74345
| epoch   1 step      290 |    290 batches | lr 0.00025 | ms/batch 538.08 | loss  2.03 | bpc   2.92944
| epoch   1 step      300 |    300 batches | lr 0.00025 | ms/batch 538.81 | loss  1.77 | bpc   2.55350 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77]
| epoch   1 step      310 |    310 batches | lr 0.00025 | ms/batch 537.76 | loss  1.68 | bpc   2.42560
| epoch   1 step      320 |    320 batches | lr 0.00025 | ms/batch 537.89 | loss  1.89 | bpc   2.72320
| epoch   1 step      330 |    330 batches | lr 0.00025 | ms/batch 537.98 | loss  2.42 | bpc   3.48818
| epoch   1 step      340 |    340 batches | lr 0.00025 | ms/batch 537.05 | loss  2.27 | bpc   3.27176
| epoch   1 step      350 |    350 batches | lr 0.00025 | ms/batch 538.78 | loss  2.28 | bpc   3.29523
| epoch   1 step      360 |    360 batches | lr 0.00025 | ms/batch 537.41 | loss  2.30 | bpc   3.32279
| epoch   1 step      370 |    370 batches | lr 0.00025 | ms/batch 537.85 | loss  2.41 | bpc   3.48355
| epoch   1 step      380 |    380 batches | lr 0.00025 | ms/batch 537.91 | loss  2.34 | bpc   3.36929
| epoch   1 step      390 |    390 batches | lr 0.00025 | ms/batch 537.77 | loss  2.49 | bpc   3.59464
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 537.75 | loss  2.34 | bpc   3.37983 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34]
| epoch   1 step      410 |    410 batches | lr 0.00025 | ms/batch 538.93 | loss  2.25 | bpc   3.24945
| epoch   1 step      420 |    420 batches | lr 0.00025 | ms/batch 538.69 | loss  2.11 | bpc   3.04614
| epoch   1 step      430 |    430 batches | lr 0.00025 | ms/batch 539.36 | loss  2.44 | bpc   3.52498
| epoch   1 step      440 |    440 batches | lr 0.00025 | ms/batch 538.06 | loss  2.21 | bpc   3.18154
| epoch   1 step      450 |    450 batches | lr 0.00025 | ms/batch 538.81 | loss  2.27 | bpc   3.28054
| epoch   1 step      460 |    460 batches | lr 0.00025 | ms/batch 537.50 | loss  2.29 | bpc   3.30089
| epoch   1 step      470 |    470 batches | lr 0.00025 | ms/batch 538.33 | loss  2.19 | bpc   3.15656
| epoch   1 step      480 |    480 batches | lr 0.00025 | ms/batch 538.05 | loss  2.30 | bpc   3.31259
| epoch   1 step      490 |    490 batches | lr 0.00025 | ms/batch 537.64 | loss  2.14 | bpc   3.08018
| epoch   1 step      500 |    500 batches | lr 0.00025 | ms/batch 537.23 | loss  2.09 | bpc   3.01361 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09]
| epoch   1 step      510 |    510 batches | lr 0.00025 | ms/batch 525.46 | loss  3.50 | bpc   5.04400
| epoch   1 step      520 |    520 batches | lr 0.00025 | ms/batch 518.10 | loss  3.27 | bpc   4.71910
| epoch   1 step      530 |    530 batches | lr 0.00025 | ms/batch 523.27 | loss  3.24 | bpc   4.67200
| epoch   1 step      540 |    540 batches | lr 0.00025 | ms/batch 525.60 | loss  2.96 | bpc   4.26689
| epoch   1 step      550 |    550 batches | lr 0.00025 | ms/batch 528.77 | loss  2.95 | bpc   4.25806
| epoch   1 step      560 |    560 batches | lr 0.00025 | ms/batch 528.78 | loss  2.66 | bpc   3.83961
| epoch   1 step      570 |    570 batches | lr 0.00025 | ms/batch 530.04 | loss  2.57 | bpc   3.71304
| epoch   1 step      580 |    580 batches | lr 0.00025 | ms/batch 530.15 | loss  2.72 | bpc   3.93128
| epoch   1 step      590 |    590 batches | lr 0.00025 | ms/batch 529.65 | loss  2.59 | bpc   3.73839
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 530.52 | loss  2.48 | bpc   3.58129 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48]
| epoch   1 step      610 |    610 batches | lr 0.00025 | ms/batch 530.04 | loss  2.31 | bpc   3.33346
| epoch   1 step      620 |    620 batches | lr 0.00025 | ms/batch 530.19 | loss  2.27 | bpc   3.26875
| epoch   1 step      630 |    630 batches | lr 0.00025 | ms/batch 530.42 | loss  2.25 | bpc   3.25151
| epoch   1 step      640 |    640 batches | lr 0.00025 | ms/batch 530.00 | loss  2.31 | bpc   3.33587
| epoch   1 step      650 |    650 batches | lr 0.00025 | ms/batch 530.15 | loss  2.44 | bpc   3.52328
| epoch   1 step      660 |    660 batches | lr 0.00025 | ms/batch 529.99 | loss  2.45 | bpc   3.53288
| epoch   1 step      670 |    670 batches | lr 0.00025 | ms/batch 530.22 | loss  2.58 | bpc   3.71857
| epoch   1 step      680 |    680 batches | lr 0.00025 | ms/batch 530.23 | loss  2.24 | bpc   3.22518
| epoch   1 step      690 |    690 batches | lr 0.00025 | ms/batch 530.40 | loss  1.99 | bpc   2.87680
| epoch   1 step      700 |    700 batches | lr 0.00025 | ms/batch 529.65 | loss  2.44 | bpc   3.51834 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44]
| epoch   1 step      710 |    710 batches | lr 0.00025 | ms/batch 530.20 | loss  2.15 | bpc   3.10581
| epoch   1 step      720 |    720 batches | lr 0.00025 | ms/batch 528.48 | loss  2.23 | bpc   3.21792
| epoch   1 step      730 |    730 batches | lr 0.00025 | ms/batch 528.02 | loss  2.32 | bpc   3.34571
| epoch   1 step      740 |    740 batches | lr 0.00025 | ms/batch 528.60 | loss  2.17 | bpc   3.13267
| epoch   1 step      750 |    750 batches | lr 0.00025 | ms/batch 528.45 | loss  2.19 | bpc   3.16436
| epoch   1 step      760 |    760 batches | lr 0.00025 | ms/batch 529.80 | loss  2.11 | bpc   3.04752
| epoch   1 step      770 |    770 batches | lr 0.00025 | ms/batch 528.56 | loss  2.19 | bpc   3.15371
| epoch   1 step      780 |    780 batches | lr 0.00025 | ms/batch 527.96 | loss  2.32 | bpc   3.34961
| epoch   1 step      790 |    790 batches | lr 0.00025 | ms/batch 531.46 | loss  2.25 | bpc   3.25244
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 529.18 | loss  2.18 | bpc   3.14804 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18]
| epoch   1 step      810 |    810 batches | lr 0.00025 | ms/batch 529.47 | loss  2.17 | bpc   3.12642
| epoch   1 step      820 |    820 batches | lr 0.00025 | ms/batch 530.28 | loss  2.21 | bpc   3.18787
| epoch   1 step      830 |    830 batches | lr 0.00025 | ms/batch 521.21 | loss  2.05 | bpc   2.96240
| epoch   1 step      840 |    840 batches | lr 0.00025 | ms/batch 514.82 | loss  2.16 | bpc   3.11110
| epoch   1 step      850 |    850 batches | lr 0.00025 | ms/batch 516.39 | loss  2.18 | bpc   3.14747
| epoch   1 step      860 |    860 batches | lr 0.00025 | ms/batch 517.22 | loss  2.16 | bpc   3.11140
| epoch   1 step      870 |    870 batches | lr 0.00025 | ms/batch 515.73 | loss  2.10 | bpc   3.02401
| epoch   1 step      880 |    880 batches | lr 0.00025 | ms/batch 516.37 | loss  2.09 | bpc   3.00975
| epoch   1 step      890 |    890 batches | lr 0.00025 | ms/batch 514.97 | loss  1.96 | bpc   2.83283
| epoch   1 step      900 |    900 batches | lr 0.00025 | ms/batch 514.25 | loss  2.24 | bpc   3.23631 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24]
| epoch   1 step      910 |    910 batches | lr 0.00025 | ms/batch 516.29 | loss  2.01 | bpc   2.89975
| epoch   1 step      920 |    920 batches | lr 0.00025 | ms/batch 514.43 | loss  2.09 | bpc   3.01508
| epoch   1 step      930 |    930 batches | lr 0.00025 | ms/batch 513.86 | loss  2.00 | bpc   2.89050
| epoch   1 step      940 |    940 batches | lr 0.00025 | ms/batch 513.14 | loss  1.98 | bpc   2.85292
| epoch   1 step      950 |    950 batches | lr 0.00025 | ms/batch 517.11 | loss  2.07 | bpc   2.98411
| epoch   1 step      960 |    960 batches | lr 0.00025 | ms/batch 513.45 | loss  2.25 | bpc   3.24906
| epoch   1 step      970 |    970 batches | lr 0.00025 | ms/batch 514.62 | loss  2.07 | bpc   2.98151
| epoch   1 step      980 |    980 batches | lr 0.00025 | ms/batch 513.32 | loss  1.95 | bpc   2.81278
| epoch   1 step      990 |    990 batches | lr 0.00025 | ms/batch 516.71 | loss  2.05 | bpc   2.95834
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 514.60 | loss  2.00 | bpc   2.89098 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0]
| epoch   1 step     1010 |   1010 batches | lr 0.00025 | ms/batch 519.63 | loss  1.89 | bpc   2.72651
| epoch   1 step     1020 |   1020 batches | lr 0.00025 | ms/batch 527.91 | loss  2.06 | bpc   2.97134
| epoch   1 step     1030 |   1030 batches | lr 0.00025 | ms/batch 529.24 | loss  1.95 | bpc   2.81907
| epoch   1 step     1040 |   1040 batches | lr 0.00025 | ms/batch 527.26 | loss  1.89 | bpc   2.73181
| epoch   1 step     1050 |   1050 batches | lr 0.00025 | ms/batch 527.51 | loss  1.99 | bpc   2.86897
| epoch   1 step     1060 |   1060 batches | lr 0.00025 | ms/batch 527.75 | loss  2.14 | bpc   3.08698
| epoch   1 step     1070 |   1070 batches | lr 0.00025 | ms/batch 527.82 | loss  2.01 | bpc   2.89333
| epoch   1 step     1080 |   1080 batches | lr 0.00025 | ms/batch 526.60 | loss  1.89 | bpc   2.72286
| epoch   1 step     1090 |   1090 batches | lr 0.00025 | ms/batch 528.18 | loss  1.89 | bpc   2.72241
| epoch   1 step     1100 |   1100 batches | lr 0.00025 | ms/batch 528.32 | loss  1.96 | bpc   2.82886 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96]
| epoch   1 step     1110 |   1110 batches | lr 0.00025 | ms/batch 528.16 | loss  1.92 | bpc   2.76726
| epoch   1 step     1120 |   1120 batches | lr 0.00025 | ms/batch 529.00 | loss  1.94 | bpc   2.80217
| epoch   1 step     1130 |   1130 batches | lr 0.00025 | ms/batch 529.27 | loss  2.09 | bpc   3.01932
| epoch   1 step     1140 |   1140 batches | lr 0.00025 | ms/batch 528.60 | loss  1.89 | bpc   2.73193
| epoch   1 step     1150 |   1150 batches | lr 0.00025 | ms/batch 528.90 | loss  1.91 | bpc   2.75090
| epoch   1 step     1160 |   1160 batches | lr 0.00025 | ms/batch 528.01 | loss  1.84 | bpc   2.65200
| epoch   1 step     1170 |   1170 batches | lr 0.00025 | ms/batch 528.71 | loss  1.97 | bpc   2.84076
| epoch   1 step     1180 |   1180 batches | lr 0.00025 | ms/batch 528.20 | loss  1.94 | bpc   2.80225
| epoch   1 step     1190 |   1190 batches | lr 0.00025 | ms/batch 527.97 | loss  1.92 | bpc   2.76720
| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 527.74 | loss  2.01 | bpc   2.90588 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96, 1.92, 1.94, 2.09, 1.89, 1.91, 1.84, 1.97, 1.94, 1.92, 2.01]
| epoch   1 step     1210 |   1210 batches | lr 0.00025 | ms/batch 526.87 | loss  1.87 | bpc   2.69819
| epoch   1 step     1220 |   1220 batches | lr 0.00025 | ms/batch 527.00 | loss  1.91 | bpc   2.75094
| epoch   1 step     1230 |   1230 batches | lr 0.00025 | ms/batch 527.20 | loss  1.92 | bpc   2.76696
| epoch   1 step     1240 |   1240 batches | lr 0.00025 | ms/batch 526.90 | loss  1.79 | bpc   2.57589
| epoch   1 step     1250 |   1250 batches | lr 0.00025 | ms/batch 526.92 | loss  2.00 | bpc   2.89016
| epoch   1 step     1260 |   1260 batches | lr 0.00025 | ms/batch 527.43 | loss  1.95 | bpc   2.81590
| epoch   1 step     1270 |   1270 batches | lr 0.00025 | ms/batch 529.35 | loss  2.07 | bpc   2.99309
| epoch   1 step     1280 |   1280 batches | lr 0.00025 | ms/batch 527.88 | loss  1.89 | bpc   2.72571
| epoch   1 step     1290 |   1290 batches | lr 0.00025 | ms/batch 528.03 | loss  1.94 | bpc   2.79789
| epoch   1 step     1300 |   1300 batches | lr 0.00025 | ms/batch 529.01 | loss  2.00 | bpc   2.87931 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96, 1.92, 1.94, 2.09, 1.89, 1.91, 1.84, 1.97, 1.94, 1.92, 2.01, 1.87, 1.91, 1.92, 1.79, 2.0, 1.95, 2.07, 1.89, 1.94, 2.0]
| epoch   1 step     1310 |   1310 batches | lr 0.00025 | ms/batch 528.64 | loss  1.93 | bpc   2.79121
| epoch   1 step     1320 |   1320 batches | lr 0.00025 | ms/batch 529.16 | loss  1.94 | bpc   2.80042
| epoch   1 step     1330 |   1330 batches | lr 0.00025 | ms/batch 529.18 | loss  1.90 | bpc   2.74263
| epoch   1 step     1340 |   1340 batches | lr 0.00025 | ms/batch 529.75 | loss  2.00 | bpc   2.88176
| epoch   1 step     1350 |   1350 batches | lr 0.00025 | ms/batch 529.89 | loss  2.02 | bpc   2.91371
| epoch   1 step     1360 |   1360 batches | lr 0.00025 | ms/batch 529.97 | loss  2.01 | bpc   2.90416
| epoch   1 step     1370 |   1370 batches | lr 0.00025 | ms/batch 530.47 | loss  1.95 | bpc   2.81902
| epoch   1 step     1380 |   1380 batches | lr 0.00025 | ms/batch 529.49 | loss  1.80 | bpc   2.59146
| epoch   1 step     1390 |   1390 batches | lr 0.00025 | ms/batch 529.75 | loss  1.77 | bpc   2.55303
| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 530.06 | loss  1.78 | bpc   2.57138 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96, 1.92, 1.94, 2.09, 1.89, 1.91, 1.84, 1.97, 1.94, 1.92, 2.01, 1.87, 1.91, 1.92, 1.79, 2.0, 1.95, 2.07, 1.89, 1.94, 2.0, 1.93, 1.94, 1.9, 2.0, 2.02, 2.01, 1.95, 1.8, 1.77, 1.78]
| epoch   1 step     1410 |   1410 batches | lr 0.00025 | ms/batch 531.86 | loss  2.07 | bpc   2.99247
| epoch   1 step     1420 |   1420 batches | lr 0.00025 | ms/batch 528.63 | loss  2.03 | bpc   2.93135
| epoch   1 step     1430 |   1430 batches | lr 0.00025 | ms/batch 528.08 | loss  1.95 | bpc   2.81735
| epoch   1 step     1440 |   1440 batches | lr 0.00025 | ms/batch 528.12 | loss  2.15 | bpc   3.10795
| epoch   1 step     1450 |   1450 batches | lr 0.00025 | ms/batch 526.92 | loss  1.90 | bpc   2.74827
| epoch   1 step     1460 |   1460 batches | lr 0.00025 | ms/batch 527.02 | loss  1.92 | bpc   2.77226
| epoch   1 step     1470 |   1470 batches | lr 0.00025 | ms/batch 526.99 | loss  1.94 | bpc   2.80278
| epoch   1 step     1480 |   1480 batches | lr 0.00025 | ms/batch 527.21 | loss  1.87 | bpc   2.70146
| epoch   1 step     1490 |   1490 batches | lr 0.00025 | ms/batch 527.83 | loss  1.94 | bpc   2.79626
| epoch   1 step     1500 |   1500 batches | lr 0.00025 | ms/batch 528.63 | loss  2.04 | bpc   2.94450 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96, 1.92, 1.94, 2.09, 1.89, 1.91, 1.84, 1.97, 1.94, 1.92, 2.01, 1.87, 1.91, 1.92, 1.79, 2.0, 1.95, 2.07, 1.89, 1.94, 2.0, 1.93, 1.94, 1.9, 2.0, 2.02, 2.01, 1.95, 1.8, 1.77, 1.78, 2.07, 2.03, 1.95, 2.15, 1.9, 1.92, 1.94, 1.87, 1.94, 2.04]
| epoch   1 step     1510 |   1510 batches | lr 0.00025 | ms/batch 530.99 | loss  1.92 | bpc   2.76983
| epoch   1 step     1520 |   1520 batches | lr 0.00025 | ms/batch 528.60 | loss  1.98 | bpc   2.85460
| epoch   1 step     1530 |   1530 batches | lr 0.00025 | ms/batch 528.48 | loss  1.82 | bpc   2.62531
| epoch   1 step     1540 |   1540 batches | lr 0.00025 | ms/batch 528.63 | loss  1.84 | bpc   2.64819
| epoch   1 step     1550 |   1550 batches | lr 0.00025 | ms/batch 529.16 | loss  2.00 | bpc   2.88692
| epoch   1 step     1560 |   1560 batches | lr 0.00025 | ms/batch 529.56 | loss  1.90 | bpc   2.73629
| epoch   1 step     1570 |   1570 batches | lr 0.00025 | ms/batch 530.17 | loss  1.79 | bpc   2.58163
| epoch   1 step     1580 |   1580 batches | lr 0.00025 | ms/batch 529.87 | loss  1.96 | bpc   2.83424
| epoch   1 step     1590 |   1590 batches | lr 0.00025 | ms/batch 529.60 | loss  2.05 | bpc   2.96267
| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 529.97 | loss  2.01 | bpc   2.90672 | current losses [4.13, 3.26, 2.83, 2.76, 2.77, 2.91, 2.82, 2.79, 2.77, 2.68, 2.84, 2.83, 2.98, 2.85, 2.73, 2.7, 2.52, 2.77, 2.48, 2.45, 2.65, 2.9, 2.71, 2.49, 2.48, 2.64, 2.45, 2.59, 2.03, 1.77, 1.68, 1.89, 2.42, 2.27, 2.28, 2.3, 2.41, 2.34, 2.49, 2.34, 2.25, 2.11, 2.44, 2.21, 2.27, 2.29, 2.19, 2.3, 2.14, 2.09, 3.5, 3.27, 3.24, 2.96, 2.95, 2.66, 2.57, 2.72, 2.59, 2.48, 2.31, 2.27, 2.25, 2.31, 2.44, 2.45, 2.58, 2.24, 1.99, 2.44, 2.15, 2.23, 2.32, 2.17, 2.19, 2.11, 2.19, 2.32, 2.25, 2.18, 2.17, 2.21, 2.05, 2.16, 2.18, 2.16, 2.1, 2.09, 1.96, 2.24, 2.01, 2.09, 2.0, 1.98, 2.07, 2.25, 2.07, 1.95, 2.05, 2.0, 1.89, 2.06, 1.95, 1.89, 1.99, 2.14, 2.01, 1.89, 1.89, 1.96, 1.92, 1.94, 2.09, 1.89, 1.91, 1.84, 1.97, 1.94, 1.92, 2.01, 1.87, 1.91, 1.92, 1.79, 2.0, 1.95, 2.07, 1.89, 1.94, 2.0, 1.93, 1.94, 1.9, 2.0, 2.02, 2.01, 1.95, 1.8, 1.77, 1.78, 2.07, 2.03, 1.95, 2.15, 1.9, 1.92, 1.94, 1.87, 1.94, 2.04, 1.92, 1.98, 1.82, 1.84, 2.0, 1.9, 1.79, 1.96, 2.05, 2.01]
----------------------------------------------------------------------------------------------------
Exiting from training early
