====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-005049
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : False
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 497.73 | loss  4.13 | bpc   5.96292
| epoch   2 step       20 |      9 batches | lr 0.00025 | ms/batch 498.58 | loss  3.30 | bpc   4.75922
| epoch   3 step       30 |      8 batches | lr 0.00025 | ms/batch 499.83 | loss  2.98 | bpc   4.29786
| epoch   4 step       40 |      7 batches | lr 0.00025 | ms/batch 500.56 | loss  2.86 | bpc   4.11918
| epoch   5 step       50 |      6 batches | lr 0.00025 | ms/batch 501.54 | loss  2.75 | bpc   3.97420
| epoch   6 step       60 |      5 batches | lr 0.00025 | ms/batch 501.81 | loss  2.64 | bpc   3.80248
| epoch   7 step       70 |      4 batches | lr 0.00025 | ms/batch 503.03 | loss  2.57 | bpc   3.70918
| epoch   8 step       80 |      3 batches | lr 0.00025 | ms/batch 503.80 | loss  2.51 | bpc   3.61832
| epoch   9 step       90 |      2 batches | lr 0.00025 | ms/batch 503.53 | loss  2.42 | bpc   3.49398
| epoch  10 step      100 |      1 batches | lr 0.00025 | ms/batch 506.08 | loss  2.41 | bpc   3.47008
| epoch  10 step      110 |     11 batches | lr 0.00025 | ms/batch 521.10 | loss  2.25 | bpc   3.24902
| epoch  11 step      120 |     10 batches | lr 0.00025 | ms/batch 507.34 | loss  2.23 | bpc   3.21093
| epoch  12 step      130 |      9 batches | lr 0.00025 | ms/batch 508.25 | loss  2.10 | bpc   3.03652
| epoch  13 step      140 |      8 batches | lr 0.00025 | ms/batch 509.09 | loss  1.98 | bpc   2.86088
| epoch  14 step      150 |      7 batches | lr 0.00025 | ms/batch 510.36 | loss  1.92 | bpc   2.76906
| epoch  15 step      160 |      6 batches | lr 0.00025 | ms/batch 511.47 | loss  1.84 | bpc   2.65701
| epoch  16 step      170 |      5 batches | lr 0.00025 | ms/batch 512.15 | loss  1.75 | bpc   2.52294
| epoch  17 step      180 |      4 batches | lr 0.00025 | ms/batch 513.72 | loss  1.67 | bpc   2.40695
| epoch  18 step      190 |      3 batches | lr 0.00025 | ms/batch 514.58 | loss  1.56 | bpc   2.25678
| epoch  19 step      200 |      2 batches | lr 0.00025 | ms/batch 514.81 | loss  1.52 | bpc   2.18780
| epoch  20 step      210 |      1 batches | lr 0.00025 | ms/batch 517.51 | loss  1.47 | bpc   2.11699
| epoch  20 step      220 |     11 batches | lr 0.00025 | ms/batch 532.80 | loss  1.27 | bpc   1.83319
| epoch  21 step      230 |     10 batches | lr 0.00025 | ms/batch 518.05 | loss  1.14 | bpc   1.65041
| epoch  22 step      240 |      9 batches | lr 0.00025 | ms/batch 519.18 | loss  1.09 | bpc   1.56597
| epoch  23 step      250 |      8 batches | lr 0.00025 | ms/batch 519.67 | loss  1.00 | bpc   1.44445
| epoch  24 step      260 |      7 batches | lr 0.00025 | ms/batch 520.88 | loss  0.95 | bpc   1.37076
| epoch  25 step      270 |      6 batches | lr 0.00025 | ms/batch 520.72 | loss  0.85 | bpc   1.22965
----------------------------------------------------------------------------------------------------
Exiting from training early
