====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-031433
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 498.53 | loss  4.13 | bpc   5.95432
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 511.76 | loss  3.27 | bpc   4.71208
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 512.83 | loss  2.84 | bpc   4.09304
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 513.89 | loss  2.77 | bpc   3.99661
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 514.32 | loss  2.77 | bpc   3.99641
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 515.54 | loss  2.89 | bpc   4.16358
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 516.29 | loss  2.82 | bpc   4.06839
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 517.84 | loss  2.79 | bpc   4.01990
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 518.13 | loss  2.76 | bpc   3.97723
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 518.73 | loss  2.68 | bpc   3.86189 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 519.60 | loss  2.83 | bpc   4.07838
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 520.70 | loss  2.83 | bpc   4.08333
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 521.64 | loss  2.98 | bpc   4.30175
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 522.28 | loss  2.87 | bpc   4.14170
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 523.27 | loss  2.75 | bpc   3.97412
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 524.58 | loss  2.72 | bpc   3.92323
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 524.95 | loss  2.55 | bpc   3.67187
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 525.65 | loss  2.76 | bpc   3.98275
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 527.40 | loss  2.50 | bpc   3.60371
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 527.94 | loss  2.43 | bpc   3.51102 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 528.10 | loss  3.74 | bpc   5.38922
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 528.41 | loss  3.54 | bpc   5.10159
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 529.24 | loss  3.13 | bpc   4.51165
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 529.46 | loss  2.99 | bpc   4.31806
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 530.91 | loss  2.66 | bpc   3.83474
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 530.98 | loss  2.88 | bpc   4.15207
| epoch   1 step      270 |    270 batches | lr 0.00025 | ms/batch 532.12 | loss  2.58 | bpc   3.71858
| epoch   1 step      280 |    280 batches | lr 0.00025 | ms/batch 534.75 | loss  2.77 | bpc   4.00241
| epoch   1 step      290 |    290 batches | lr 0.00025 | ms/batch 534.76 | loss  2.23 | bpc   3.21329
| epoch   1 step      300 |    300 batches | lr 0.00025 | ms/batch 531.77 | loss  1.98 | bpc   2.86015 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98]
| epoch   1 step      310 |    310 batches | lr 0.00025 | ms/batch 533.68 | loss  1.86 | bpc   2.68174
| epoch   1 step      320 |    320 batches | lr 0.00025 | ms/batch 532.49 | loss  2.04 | bpc   2.94651
| epoch   1 step      330 |    330 batches | lr 0.00025 | ms/batch 533.51 | loss  2.54 | bpc   3.65725
| epoch   1 step      340 |    340 batches | lr 0.00025 | ms/batch 533.24 | loss  2.35 | bpc   3.39350
| epoch   1 step      350 |    350 batches | lr 0.00025 | ms/batch 535.08 | loss  2.31 | bpc   3.32776
| epoch   1 step      360 |    360 batches | lr 0.00025 | ms/batch 535.45 | loss  2.36 | bpc   3.40755
| epoch   1 step      370 |    370 batches | lr 0.00025 | ms/batch 532.89 | loss  2.51 | bpc   3.62791
| epoch   1 step      380 |    380 batches | lr 0.00025 | ms/batch 533.45 | loss  2.47 | bpc   3.55666
| epoch   1 step      390 |    390 batches | lr 0.00025 | ms/batch 536.81 | loss  2.66 | bpc   3.83755
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 533.48 | loss  2.43 | bpc   3.50418 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43]
| epoch   1 step      410 |    410 batches | lr 0.00025 | ms/batch 536.33 | loss  2.34 | bpc   3.37380
| epoch   1 step      420 |    420 batches | lr 0.00025 | ms/batch 533.94 | loss  2.22 | bpc   3.19608
| epoch   1 step      430 |    430 batches | lr 0.00025 | ms/batch 537.29 | loss  2.52 | bpc   3.63921
| epoch   1 step      440 |    440 batches | lr 0.00025 | ms/batch 538.54 | loss  2.29 | bpc   3.30086
| epoch   1 step      450 |    450 batches | lr 0.00025 | ms/batch 535.27 | loss  2.38 | bpc   3.43289
| epoch   1 step      460 |    460 batches | lr 0.00025 | ms/batch 534.23 | loss  2.39 | bpc   3.44846
| epoch   1 step      470 |    470 batches | lr 0.00025 | ms/batch 533.11 | loss  2.28 | bpc   3.28447
| epoch   1 step      480 |    480 batches | lr 0.00025 | ms/batch 535.70 | loss  2.43 | bpc   3.51187
| epoch   1 step      490 |    490 batches | lr 0.00025 | ms/batch 531.98 | loss  2.27 | bpc   3.27269
| epoch   1 step      500 |    500 batches | lr 0.00025 | ms/batch 533.69 | loss  2.21 | bpc   3.18149 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21]
| epoch   1 step      510 |    510 batches | lr 0.00025 | ms/batch 537.46 | loss  2.23 | bpc   3.21976
| epoch   1 step      520 |    520 batches | lr 0.00025 | ms/batch 536.20 | loss  2.24 | bpc   3.23853
| epoch   1 step      530 |    530 batches | lr 0.00025 | ms/batch 534.43 | loss  2.18 | bpc   3.14707
| epoch   1 step      540 |    540 batches | lr 0.00025 | ms/batch 533.18 | loss  2.21 | bpc   3.19501
| epoch   1 step      550 |    550 batches | lr 0.00025 | ms/batch 535.89 | loss  2.31 | bpc   3.33551
| epoch   1 step      560 |    560 batches | lr 0.00025 | ms/batch 532.21 | loss  2.17 | bpc   3.12457
| epoch   1 step      570 |    570 batches | lr 0.00025 | ms/batch 533.56 | loss  2.25 | bpc   3.25239
| epoch   1 step      580 |    580 batches | lr 0.00025 | ms/batch 533.59 | loss  2.33 | bpc   3.36397
| epoch   1 step      590 |    590 batches | lr 0.00025 | ms/batch 533.54 | loss  2.25 | bpc   3.24826
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 535.21 | loss  2.23 | bpc   3.21348 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23]
| epoch   1 step      610 |    610 batches | lr 0.00025 | ms/batch 533.24 | loss  2.12 | bpc   3.05852
| epoch   1 step      620 |    620 batches | lr 0.00025 | ms/batch 537.92 | loss  2.10 | bpc   3.03186
| epoch   1 step      630 |    630 batches | lr 0.00025 | ms/batch 534.68 | loss  2.12 | bpc   3.05187
| epoch   1 step      640 |    640 batches | lr 0.00025 | ms/batch 536.83 | loss  2.15 | bpc   3.10827
| epoch   1 step      650 |    650 batches | lr 0.00025 | ms/batch 534.31 | loss  2.23 | bpc   3.22108
| epoch   1 step      660 |    660 batches | lr 0.00025 | ms/batch 532.58 | loss  2.35 | bpc   3.39114
| epoch   1 step      670 |    670 batches | lr 0.00025 | ms/batch 531.61 | loss  2.26 | bpc   3.26399
| epoch   1 step      680 |    680 batches | lr 0.00025 | ms/batch 531.88 | loss  2.01 | bpc   2.89514
| epoch   1 step      690 |    690 batches | lr 0.00025 | ms/batch 531.87 | loss  1.76 | bpc   2.54433
| epoch   1 step      700 |    700 batches | lr 0.00025 | ms/batch 532.09 | loss  2.30 | bpc   3.32406 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3]
| epoch   1 step      710 |    710 batches | lr 0.00025 | ms/batch 535.89 | loss  1.94 | bpc   2.79908
| epoch   1 step      720 |    720 batches | lr 0.00025 | ms/batch 533.02 | loss  2.10 | bpc   3.03414
| epoch   1 step      730 |    730 batches | lr 0.00025 | ms/batch 537.79 | loss  2.19 | bpc   3.15882
| epoch   1 step      740 |    740 batches | lr 0.00025 | ms/batch 531.15 | loss  2.03 | bpc   2.92422
| epoch   1 step      750 |    750 batches | lr 0.00025 | ms/batch 533.87 | loss  2.07 | bpc   2.99111
| epoch   1 step      760 |    760 batches | lr 0.00025 | ms/batch 534.22 | loss  1.98 | bpc   2.86217
| epoch   1 step      770 |    770 batches | lr 0.00025 | ms/batch 535.74 | loss  2.08 | bpc   3.00436
| epoch   1 step      780 |    780 batches | lr 0.00025 | ms/batch 534.17 | loss  2.22 | bpc   3.20389
| epoch   1 step      790 |    790 batches | lr 0.00025 | ms/batch 532.67 | loss  2.17 | bpc   3.13120
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 533.58 | loss  2.08 | bpc   2.99701 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08]
| epoch   1 step      810 |    810 batches | lr 0.00025 | ms/batch 532.42 | loss  2.08 | bpc   3.00463
| epoch   1 step      820 |    820 batches | lr 0.00025 | ms/batch 537.26 | loss  2.14 | bpc   3.08880
| epoch   1 step      830 |    830 batches | lr 0.00025 | ms/batch 533.21 | loss  1.96 | bpc   2.83164
| epoch   1 step      840 |    840 batches | lr 0.00025 | ms/batch 532.65 | loss  2.09 | bpc   3.01656
| epoch   1 step      850 |    850 batches | lr 0.00025 | ms/batch 535.18 | loss  2.16 | bpc   3.11393
| epoch   1 step      860 |    860 batches | lr 0.00025 | ms/batch 533.65 | loss  2.09 | bpc   3.01046
| epoch   1 step      870 |    870 batches | lr 0.00025 | ms/batch 535.39 | loss  2.01 | bpc   2.90210
| epoch   1 step      880 |    880 batches | lr 0.00025 | ms/batch 534.31 | loss  1.99 | bpc   2.87248
| epoch   1 step      890 |    890 batches | lr 0.00025 | ms/batch 531.79 | loss  1.88 | bpc   2.71181
| epoch   1 step      900 |    900 batches | lr 0.00025 | ms/batch 533.48 | loss  2.18 | bpc   3.15018 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18]
| epoch   1 step      910 |    910 batches | lr 0.00025 | ms/batch 531.83 | loss  1.91 | bpc   2.75138
| epoch   1 step      920 |    920 batches | lr 0.00025 | ms/batch 533.33 | loss  2.03 | bpc   2.93339
| epoch   1 step      930 |    930 batches | lr 0.00025 | ms/batch 530.66 | loss  1.96 | bpc   2.82240
| epoch   1 step      940 |    940 batches | lr 0.00025 | ms/batch 532.64 | loss  1.92 | bpc   2.76448
| epoch   1 step      950 |    950 batches | lr 0.00025 | ms/batch 530.57 | loss  2.01 | bpc   2.89480
| epoch   1 step      960 |    960 batches | lr 0.00025 | ms/batch 533.48 | loss  2.19 | bpc   3.15621
| epoch   1 step      970 |    970 batches | lr 0.00025 | ms/batch 530.69 | loss  2.01 | bpc   2.90099
| epoch   1 step      980 |    980 batches | lr 0.00025 | ms/batch 534.80 | loss  1.89 | bpc   2.72677
| epoch   1 step      990 |    990 batches | lr 0.00025 | ms/batch 533.20 | loss  1.99 | bpc   2.86925
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 530.87 | loss  1.94 | bpc   2.79701 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94]
| epoch   1 step     1010 |   1010 batches | lr 0.00025 | ms/batch 535.28 | loss  1.83 | bpc   2.63851
| epoch   1 step     1020 |   1020 batches | lr 0.00025 | ms/batch 533.92 | loss  2.01 | bpc   2.90452
| epoch   1 step     1030 |   1030 batches | lr 0.00025 | ms/batch 534.75 | loss  1.91 | bpc   2.75799
| epoch   1 step     1040 |   1040 batches | lr 0.00025 | ms/batch 537.15 | loss  1.86 | bpc   2.67756
| epoch   1 step     1050 |   1050 batches | lr 0.00025 | ms/batch 537.26 | loss  1.94 | bpc   2.80249
| epoch   1 step     1060 |   1060 batches | lr 0.00025 | ms/batch 535.83 | loss  2.12 | bpc   3.05204
| epoch   1 step     1070 |   1070 batches | lr 0.00025 | ms/batch 535.71 | loss  1.98 | bpc   2.85073
| epoch   1 step     1080 |   1080 batches | lr 0.00025 | ms/batch 535.34 | loss  1.87 | bpc   2.69830
| epoch   1 step     1090 |   1090 batches | lr 0.00025 | ms/batch 532.32 | loss  1.80 | bpc   2.59508
| epoch   1 step     1100 |   1100 batches | lr 0.00025 | ms/batch 533.42 | loss  1.91 | bpc   2.75210 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91]
| epoch   1 step     1110 |   1110 batches | lr 0.00025 | ms/batch 534.42 | loss  1.84 | bpc   2.66029
| epoch   1 step     1120 |   1120 batches | lr 0.00025 | ms/batch 531.58 | loss  1.87 | bpc   2.70089
| epoch   1 step     1130 |   1130 batches | lr 0.00025 | ms/batch 534.20 | loss  2.03 | bpc   2.93259
| epoch   1 step     1140 |   1140 batches | lr 0.00025 | ms/batch 532.02 | loss  1.83 | bpc   2.64227
| epoch   1 step     1150 |   1150 batches | lr 0.00025 | ms/batch 533.62 | loss  1.85 | bpc   2.66454
| epoch   1 step     1160 |   1160 batches | lr 0.00025 | ms/batch 533.34 | loss  1.81 | bpc   2.60488
| epoch   1 step     1170 |   1170 batches | lr 0.00025 | ms/batch 534.18 | loss  1.90 | bpc   2.73997
| epoch   1 step     1180 |   1180 batches | lr 0.00025 | ms/batch 530.87 | loss  1.85 | bpc   2.67177
| epoch   1 step     1190 |   1190 batches | lr 0.00025 | ms/batch 534.94 | loss  1.87 | bpc   2.70323
| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 532.23 | loss  1.96 | bpc   2.83335 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96]
| epoch   1 step     1210 |   1210 batches | lr 0.00025 | ms/batch 533.39 | loss  1.82 | bpc   2.62166
| epoch   1 step     1220 |   1220 batches | lr 0.00025 | ms/batch 534.29 | loss  1.86 | bpc   2.68007
| epoch   1 step     1230 |   1230 batches | lr 0.00025 | ms/batch 533.90 | loss  1.88 | bpc   2.71533
| epoch   1 step     1240 |   1240 batches | lr 0.00025 | ms/batch 534.29 | loss  1.74 | bpc   2.51063
| epoch   1 step     1250 |   1250 batches | lr 0.00025 | ms/batch 534.47 | loss  1.97 | bpc   2.84142
| epoch   1 step     1260 |   1260 batches | lr 0.00025 | ms/batch 531.88 | loss  1.90 | bpc   2.74134
| epoch   1 step     1270 |   1270 batches | lr 0.00025 | ms/batch 534.90 | loss  2.03 | bpc   2.92511
| epoch   1 step     1280 |   1280 batches | lr 0.00025 | ms/batch 518.10 | loss  1.83 | bpc   2.64476
| epoch   1 step     1290 |   1290 batches | lr 0.00025 | ms/batch 518.11 | loss  1.90 | bpc   2.74732
| epoch   1 step     1300 |   1300 batches | lr 0.00025 | ms/batch 519.49 | loss  1.94 | bpc   2.80508 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96, 1.82, 1.86, 1.88, 1.74, 1.97, 1.9, 2.03, 1.83, 1.9, 1.94]
| epoch   1 step     1310 |   1310 batches | lr 0.00025 | ms/batch 516.61 | loss  1.91 | bpc   2.75244
| epoch   1 step     1320 |   1320 batches | lr 0.00025 | ms/batch 517.43 | loss  1.93 | bpc   2.78429
| epoch   1 step     1330 |   1330 batches | lr 0.00025 | ms/batch 517.96 | loss  1.88 | bpc   2.71191
| epoch   1 step     1340 |   1340 batches | lr 0.00025 | ms/batch 516.13 | loss  1.95 | bpc   2.82032
| epoch   1 step     1350 |   1350 batches | lr 0.00025 | ms/batch 517.05 | loss  1.99 | bpc   2.87199
| epoch   1 step     1360 |   1360 batches | lr 0.00025 | ms/batch 519.35 | loss  1.99 | bpc   2.86901
| epoch   1 step     1370 |   1370 batches | lr 0.00025 | ms/batch 518.02 | loss  1.90 | bpc   2.74789
| epoch   1 step     1380 |   1380 batches | lr 0.00025 | ms/batch 516.65 | loss  1.78 | bpc   2.56748
| epoch   1 step     1390 |   1390 batches | lr 0.00025 | ms/batch 516.97 | loss  1.72 | bpc   2.47616
| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 518.50 | loss  1.74 | bpc   2.50638 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96, 1.82, 1.86, 1.88, 1.74, 1.97, 1.9, 2.03, 1.83, 1.9, 1.94, 1.91, 1.93, 1.88, 1.95, 1.99, 1.99, 1.9, 1.78, 1.72, 1.74]
| epoch   1 step     1410 |   1410 batches | lr 0.00025 | ms/batch 518.29 | loss  2.02 | bpc   2.90707
| epoch   1 step     1420 |   1420 batches | lr 0.00025 | ms/batch 515.70 | loss  1.99 | bpc   2.86973
| epoch   1 step     1430 |   1430 batches | lr 0.00025 | ms/batch 518.05 | loss  1.92 | bpc   2.77637
| epoch   1 step     1440 |   1440 batches | lr 0.00025 | ms/batch 517.41 | loss  2.12 | bpc   3.05938
| epoch   1 step     1450 |   1450 batches | lr 0.00025 | ms/batch 516.00 | loss  1.90 | bpc   2.73665
| epoch   1 step     1460 |   1460 batches | lr 0.00025 | ms/batch 520.76 | loss  1.89 | bpc   2.73252
| epoch   1 step     1470 |   1470 batches | lr 0.00025 | ms/batch 517.82 | loss  1.92 | bpc   2.76283
| epoch   1 step     1480 |   1480 batches | lr 0.00025 | ms/batch 519.65 | loss  1.85 | bpc   2.66468
| epoch   1 step     1490 |   1490 batches | lr 0.00025 | ms/batch 518.46 | loss  1.92 | bpc   2.77240
| epoch   1 step     1500 |   1500 batches | lr 0.00025 | ms/batch 518.74 | loss  2.00 | bpc   2.88702 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96, 1.82, 1.86, 1.88, 1.74, 1.97, 1.9, 2.03, 1.83, 1.9, 1.94, 1.91, 1.93, 1.88, 1.95, 1.99, 1.99, 1.9, 1.78, 1.72, 1.74, 2.02, 1.99, 1.92, 2.12, 1.9, 1.89, 1.92, 1.85, 1.92, 2.0]
| epoch   1 step     1510 |   1510 batches | lr 0.00025 | ms/batch 520.49 | loss  1.87 | bpc   2.70505
| epoch   1 step     1520 |   1520 batches | lr 0.00025 | ms/batch 516.56 | loss  1.97 | bpc   2.84928
| epoch   1 step     1530 |   1530 batches | lr 0.00025 | ms/batch 515.29 | loss  1.79 | bpc   2.58163
| epoch   1 step     1540 |   1540 batches | lr 0.00025 | ms/batch 518.95 | loss  1.80 | bpc   2.59339
| epoch   1 step     1550 |   1550 batches | lr 0.00025 | ms/batch 518.12 | loss  1.98 | bpc   2.85246
| epoch   1 step     1560 |   1560 batches | lr 0.00025 | ms/batch 516.12 | loss  1.89 | bpc   2.72966
| epoch   1 step     1570 |   1570 batches | lr 0.00025 | ms/batch 519.24 | loss  1.75 | bpc   2.52971
| epoch   1 step     1580 |   1580 batches | lr 0.00025 | ms/batch 518.54 | loss  1.94 | bpc   2.80414
| epoch   1 step     1590 |   1590 batches | lr 0.00025 | ms/batch 520.50 | loss  2.03 | bpc   2.92161
| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 517.80 | loss  1.96 | bpc   2.83380 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96, 1.82, 1.86, 1.88, 1.74, 1.97, 1.9, 2.03, 1.83, 1.9, 1.94, 1.91, 1.93, 1.88, 1.95, 1.99, 1.99, 1.9, 1.78, 1.72, 1.74, 2.02, 1.99, 1.92, 2.12, 1.9, 1.89, 1.92, 1.85, 1.92, 2.0, 1.87, 1.97, 1.79, 1.8, 1.98, 1.89, 1.75, 1.94, 2.03, 1.96]
| epoch   1 step     1610 |   1610 batches | lr 0.00025 | ms/batch 518.80 | loss  2.03 | bpc   2.93136
| epoch   1 step     1620 |   1620 batches | lr 0.00025 | ms/batch 518.58 | loss  2.06 | bpc   2.96816
| epoch   1 step     1630 |   1630 batches | lr 0.00025 | ms/batch 516.97 | loss  1.93 | bpc   2.78478
| epoch   1 step     1640 |   1640 batches | lr 0.00025 | ms/batch 517.89 | loss  1.93 | bpc   2.77868
| epoch   1 step     1650 |   1650 batches | lr 0.00025 | ms/batch 516.45 | loss  1.77 | bpc   2.55734
| epoch   1 step     1660 |   1660 batches | lr 0.00025 | ms/batch 518.89 | loss  1.87 | bpc   2.69287
| epoch   1 step     1670 |   1670 batches | lr 0.00025 | ms/batch 518.04 | loss  2.03 | bpc   2.92526
| epoch   1 step     1680 |   1680 batches | lr 0.00025 | ms/batch 515.55 | loss  1.73 | bpc   2.49055
| epoch   1 step     1690 |   1690 batches | lr 0.00025 | ms/batch 516.81 | loss  1.80 | bpc   2.59707
| epoch   1 step     1700 |   1700 batches | lr 0.00025 | ms/batch 516.93 | loss  1.95 | bpc   2.80895 | current losses [4.13, 3.27, 2.84, 2.77, 2.77, 2.89, 2.82, 2.79, 2.76, 2.68, 2.83, 2.83, 2.98, 2.87, 2.75, 2.72, 2.55, 2.76, 2.5, 2.43, 3.74, 3.54, 3.13, 2.99, 2.66, 2.88, 2.58, 2.77, 2.23, 1.98, 1.86, 2.04, 2.54, 2.35, 2.31, 2.36, 2.51, 2.47, 2.66, 2.43, 2.34, 2.22, 2.52, 2.29, 2.38, 2.39, 2.28, 2.43, 2.27, 2.21, 2.23, 2.24, 2.18, 2.21, 2.31, 2.17, 2.25, 2.33, 2.25, 2.23, 2.12, 2.1, 2.12, 2.15, 2.23, 2.35, 2.26, 2.01, 1.76, 2.3, 1.94, 2.1, 2.19, 2.03, 2.07, 1.98, 2.08, 2.22, 2.17, 2.08, 2.08, 2.14, 1.96, 2.09, 2.16, 2.09, 2.01, 1.99, 1.88, 2.18, 1.91, 2.03, 1.96, 1.92, 2.01, 2.19, 2.01, 1.89, 1.99, 1.94, 1.83, 2.01, 1.91, 1.86, 1.94, 2.12, 1.98, 1.87, 1.8, 1.91, 1.84, 1.87, 2.03, 1.83, 1.85, 1.81, 1.9, 1.85, 1.87, 1.96, 1.82, 1.86, 1.88, 1.74, 1.97, 1.9, 2.03, 1.83, 1.9, 1.94, 1.91, 1.93, 1.88, 1.95, 1.99, 1.99, 1.9, 1.78, 1.72, 1.74, 2.02, 1.99, 1.92, 2.12, 1.9, 1.89, 1.92, 1.85, 1.92, 2.0, 1.87, 1.97, 1.79, 1.8, 1.98, 1.89, 1.75, 1.94, 2.03, 1.96, 2.03, 2.06, 1.93, 1.93, 1.77, 1.87, 2.03, 1.73, 1.8, 1.95]
----------------------------------------------------------------------------------------------------
Exiting from training early
