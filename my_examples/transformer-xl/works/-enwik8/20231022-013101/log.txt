====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-013101
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 503.90 | loss  4.41 | bpc   6.36085
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 508.72 | loss  3.48 | bpc   5.01656
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 508.45 | loss  3.31 | bpc   4.77793
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 508.06 | loss  3.33 | bpc   4.79801
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 508.00 | loss  3.37 | bpc   4.86579
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 507.74 | loss  3.50 | bpc   5.04889
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 508.28 | loss  3.42 | bpc   4.93580
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 508.38 | loss  3.52 | bpc   5.07571
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 509.79 | loss  3.43 | bpc   4.94414
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 509.41 | loss  3.34 | bpc   4.82547 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 509.70 | loss  3.53 | bpc   5.09395
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 510.50 | loss  3.53 | bpc   5.09586
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 510.62 | loss  3.74 | bpc   5.39252
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 511.20 | loss  3.54 | bpc   5.11326
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 511.30 | loss  3.34 | bpc   4.82114
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 511.07 | loss  3.37 | bpc   4.86633
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 512.01 | loss  3.22 | bpc   4.64611
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 512.05 | loss  3.47 | bpc   4.99956
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 513.54 | loss  3.22 | bpc   4.65100
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 513.73 | loss  3.24 | bpc   4.67241 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 516.88 | loss  3.60 | bpc   5.19866
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 510.30 | loss  3.61 | bpc   5.20982
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 510.96 | loss  3.57 | bpc   5.14445
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 509.36 | loss  3.61 | bpc   5.20714
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 507.73 | loss  3.23 | bpc   4.66469
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 508.63 | loss  3.57 | bpc   5.15100
| epoch   1 step      270 |    270 batches | lr 0.00025 | ms/batch 509.07 | loss  3.33 | bpc   4.80343
| epoch   1 step      280 |    280 batches | lr 0.00025 | ms/batch 510.46 | loss  3.66 | bpc   5.28717
| epoch   1 step      290 |    290 batches | lr 0.00025 | ms/batch 509.15 | loss  3.41 | bpc   4.92602
| epoch   1 step      300 |    300 batches | lr 0.00025 | ms/batch 509.85 | loss  3.37 | bpc   4.86827 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37]
| epoch   1 step      310 |    310 batches | lr 0.00025 | ms/batch 508.42 | loss  3.37 | bpc   4.85799
| epoch   1 step      320 |    320 batches | lr 0.00025 | ms/batch 508.38 | loss  3.44 | bpc   4.96608
| epoch   1 step      330 |    330 batches | lr 0.00025 | ms/batch 508.81 | loss  3.56 | bpc   5.14245
| epoch   1 step      340 |    340 batches | lr 0.00025 | ms/batch 508.14 | loss  3.45 | bpc   4.98071
| epoch   1 step      350 |    350 batches | lr 0.00025 | ms/batch 508.63 | loss  3.61 | bpc   5.20413
| epoch   1 step      360 |    360 batches | lr 0.00025 | ms/batch 507.79 | loss  3.35 | bpc   4.83630
| epoch   1 step      370 |    370 batches | lr 0.00025 | ms/batch 508.33 | loss  3.64 | bpc   5.25320
| epoch   1 step      380 |    380 batches | lr 0.00025 | ms/batch 508.63 | loss  3.71 | bpc   5.35305
| epoch   1 step      390 |    390 batches | lr 0.00025 | ms/batch 510.86 | loss  4.04 | bpc   5.83432
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 507.83 | loss  3.55 | bpc   5.12212 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55]
| epoch   1 step      410 |    410 batches | lr 0.00025 | ms/batch 508.39 | loss  3.37 | bpc   4.85618
| epoch   1 step      420 |    420 batches | lr 0.00025 | ms/batch 507.89 | loss  3.46 | bpc   4.99241
| epoch   1 step      430 |    430 batches | lr 0.00025 | ms/batch 507.86 | loss  3.73 | bpc   5.38099
| epoch   1 step      440 |    440 batches | lr 0.00025 | ms/batch 507.16 | loss  3.34 | bpc   4.81655
| epoch   1 step      450 |    450 batches | lr 0.00025 | ms/batch 507.79 | loss  3.57 | bpc   5.15753
| epoch   1 step      460 |    460 batches | lr 0.00025 | ms/batch 507.92 | loss  3.50 | bpc   5.05237
| epoch   1 step      470 |    470 batches | lr 0.00025 | ms/batch 507.63 | loss  3.28 | bpc   4.72716
| epoch   1 step      480 |    480 batches | lr 0.00025 | ms/batch 508.70 | loss  3.62 | bpc   5.22815
| epoch   1 step      490 |    490 batches | lr 0.00025 | ms/batch 508.83 | loss  3.30 | bpc   4.76067
| epoch   1 step      500 |    500 batches | lr 0.00025 | ms/batch 509.20 | loss  3.21 | bpc   4.62513 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21]
| epoch   1 step      510 |    510 batches | lr 0.00025 | ms/batch 509.62 | loss  3.27 | bpc   4.71284
| epoch   1 step      520 |    520 batches | lr 0.00025 | ms/batch 509.00 | loss  3.26 | bpc   4.70231
| epoch   1 step      530 |    530 batches | lr 0.00025 | ms/batch 508.94 | loss  3.32 | bpc   4.79637
| epoch   1 step      540 |    540 batches | lr 0.00025 | ms/batch 509.14 | loss  3.33 | bpc   4.80261
| epoch   1 step      550 |    550 batches | lr 0.00025 | ms/batch 509.22 | loss  3.68 | bpc   5.30795
| epoch   1 step      560 |    560 batches | lr 0.00025 | ms/batch 508.25 | loss  3.36 | bpc   4.84900
| epoch   1 step      570 |    570 batches | lr 0.00025 | ms/batch 509.44 | loss  3.45 | bpc   4.97080
| epoch   1 step      580 |    580 batches | lr 0.00025 | ms/batch 509.75 | loss  3.67 | bpc   5.28849
| epoch   1 step      590 |    590 batches | lr 0.00025 | ms/batch 510.30 | loss  3.72 | bpc   5.36213
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 509.39 | loss  3.41 | bpc   4.92510 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41]
| epoch   1 step      610 |    610 batches | lr 0.00025 | ms/batch 509.30 | loss  3.17 | bpc   4.57821
| epoch   1 step      620 |    620 batches | lr 0.00025 | ms/batch 508.79 | loss  3.29 | bpc   4.73937
| epoch   1 step      630 |    630 batches | lr 0.00025 | ms/batch 508.85 | loss  3.24 | bpc   4.67256
| epoch   1 step      640 |    640 batches | lr 0.00025 | ms/batch 509.40 | loss  3.41 | bpc   4.91343
| epoch   1 step      650 |    650 batches | lr 0.00025 | ms/batch 509.29 | loss  3.43 | bpc   4.94230
| epoch   1 step      660 |    660 batches | lr 0.00025 | ms/batch 509.45 | loss  3.68 | bpc   5.30464
| epoch   1 step      670 |    670 batches | lr 0.00025 | ms/batch 509.56 | loss  3.79 | bpc   5.46661
| epoch   1 step      680 |    680 batches | lr 0.00025 | ms/batch 511.43 | loss  3.67 | bpc   5.29451
| epoch   1 step      690 |    690 batches | lr 0.00025 | ms/batch 509.11 | loss  3.48 | bpc   5.01813
| epoch   1 step      700 |    700 batches | lr 0.00025 | ms/batch 510.05 | loss  3.64 | bpc   5.25556 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64]
| epoch   1 step      710 |    710 batches | lr 0.00025 | ms/batch 509.35 | loss  3.73 | bpc   5.38665
| epoch   1 step      720 |    720 batches | lr 0.00025 | ms/batch 509.50 | loss  3.47 | bpc   5.01134
| epoch   1 step      730 |    730 batches | lr 0.00025 | ms/batch 510.59 | loss  3.56 | bpc   5.13290
| epoch   1 step      740 |    740 batches | lr 0.00025 | ms/batch 509.94 | loss  3.36 | bpc   4.84028
| epoch   1 step      750 |    750 batches | lr 0.00025 | ms/batch 509.99 | loss  3.18 | bpc   4.59164
| epoch   1 step      760 |    760 batches | lr 0.00025 | ms/batch 510.79 | loss  3.42 | bpc   4.92957
| epoch   1 step      770 |    770 batches | lr 0.00025 | ms/batch 514.09 | loss  3.42 | bpc   4.93283
| epoch   1 step      780 |    780 batches | lr 0.00025 | ms/batch 514.37 | loss  3.69 | bpc   5.32266
| epoch   1 step      790 |    790 batches | lr 0.00025 | ms/batch 511.69 | loss  3.68 | bpc   5.30569
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 510.09 | loss  3.31 | bpc   4.77323 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31]
| epoch   1 step      810 |    810 batches | lr 0.00025 | ms/batch 510.52 | loss  3.31 | bpc   4.77862
| epoch   1 step      820 |    820 batches | lr 0.00025 | ms/batch 511.32 | loss  3.50 | bpc   5.05615
| epoch   1 step      830 |    830 batches | lr 0.00025 | ms/batch 510.08 | loss  3.44 | bpc   4.95767
| epoch   1 step      840 |    840 batches | lr 0.00025 | ms/batch 508.85 | loss  3.37 | bpc   4.86212
| epoch   1 step      850 |    850 batches | lr 0.00025 | ms/batch 509.13 | loss  3.46 | bpc   4.99054
| epoch   1 step      860 |    860 batches | lr 0.00025 | ms/batch 508.39 | loss  3.41 | bpc   4.92173
| epoch   1 step      870 |    870 batches | lr 0.00025 | ms/batch 509.01 | loss  3.52 | bpc   5.07328
| epoch   1 step      880 |    880 batches | lr 0.00025 | ms/batch 508.83 | loss  3.44 | bpc   4.96025
| epoch   1 step      890 |    890 batches | lr 0.00025 | ms/batch 508.50 | loss  3.51 | bpc   5.05806
| epoch   1 step      900 |    900 batches | lr 0.00025 | ms/batch 508.27 | loss  3.46 | bpc   4.99775 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46]
| epoch   1 step      910 |    910 batches | lr 0.00025 | ms/batch 508.81 | loss  3.63 | bpc   5.23677
| epoch   1 step      920 |    920 batches | lr 0.00025 | ms/batch 508.80 | loss  3.35 | bpc   4.83147
| epoch   1 step      930 |    930 batches | lr 0.00025 | ms/batch 509.53 | loss  3.26 | bpc   4.70238
| epoch   1 step      940 |    940 batches | lr 0.00025 | ms/batch 507.58 | loss  3.52 | bpc   5.07172
| epoch   1 step      950 |    950 batches | lr 0.00025 | ms/batch 508.46 | loss  3.32 | bpc   4.79230
| epoch   1 step      960 |    960 batches | lr 0.00025 | ms/batch 508.57 | loss  3.71 | bpc   5.35727
| epoch   1 step      970 |    970 batches | lr 0.00025 | ms/batch 508.04 | loss  3.32 | bpc   4.78298
| epoch   1 step      980 |    980 batches | lr 0.00025 | ms/batch 508.09 | loss  3.29 | bpc   4.74067
| epoch   1 step      990 |    990 batches | lr 0.00025 | ms/batch 508.06 | loss  3.35 | bpc   4.82754
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 507.79 | loss  3.41 | bpc   4.91508 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41]
| epoch   1 step     1010 |   1010 batches | lr 0.00025 | ms/batch 508.43 | loss  3.30 | bpc   4.76299
| epoch   1 step     1020 |   1020 batches | lr 0.00025 | ms/batch 508.41 | loss  3.51 | bpc   5.06315
| epoch   1 step     1030 |   1030 batches | lr 0.00025 | ms/batch 507.16 | loss  3.32 | bpc   4.79143
| epoch   1 step     1040 |   1040 batches | lr 0.00025 | ms/batch 507.43 | loss  3.35 | bpc   4.83500
| epoch   1 step     1050 |   1050 batches | lr 0.00025 | ms/batch 507.85 | loss  3.30 | bpc   4.75747
| epoch   1 step     1060 |   1060 batches | lr 0.00025 | ms/batch 507.48 | loss  3.36 | bpc   4.84634
| epoch   1 step     1070 |   1070 batches | lr 0.00025 | ms/batch 507.55 | loss  3.36 | bpc   4.84746
| epoch   1 step     1080 |   1080 batches | lr 0.00025 | ms/batch 507.97 | loss  3.52 | bpc   5.08061
| epoch   1 step     1090 |   1090 batches | lr 0.00025 | ms/batch 508.54 | loss  3.54 | bpc   5.11098
| epoch   1 step     1100 |   1100 batches | lr 0.00025 | ms/batch 508.51 | loss  3.33 | bpc   4.80482 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33]
| epoch   1 step     1110 |   1110 batches | lr 0.00025 | ms/batch 509.54 | loss  3.25 | bpc   4.69006
| epoch   1 step     1120 |   1120 batches | lr 0.00025 | ms/batch 509.17 | loss  3.30 | bpc   4.76730
| epoch   1 step     1130 |   1130 batches | lr 0.00025 | ms/batch 510.45 | loss  3.51 | bpc   5.06017
| epoch   1 step     1140 |   1140 batches | lr 0.00025 | ms/batch 509.56 | loss  3.30 | bpc   4.76353
| epoch   1 step     1150 |   1150 batches | lr 0.00025 | ms/batch 509.73 | loss  3.15 | bpc   4.55087
| epoch   1 step     1160 |   1160 batches | lr 0.00025 | ms/batch 509.68 | loss  3.21 | bpc   4.63524
| epoch   1 step     1170 |   1170 batches | lr 0.00025 | ms/batch 509.71 | loss  3.30 | bpc   4.76436
| epoch   1 step     1180 |   1180 batches | lr 0.00025 | ms/batch 509.24 | loss  3.42 | bpc   4.93113
| epoch   1 step     1190 |   1190 batches | lr 0.00025 | ms/batch 507.98 | loss  3.23 | bpc   4.66458
| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 509.06 | loss  3.64 | bpc   5.24693 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64]
| epoch   1 step     1210 |   1210 batches | lr 0.00025 | ms/batch 509.64 | loss  3.17 | bpc   4.57694
| epoch   1 step     1220 |   1220 batches | lr 0.00025 | ms/batch 507.56 | loss  3.30 | bpc   4.75502
| epoch   1 step     1230 |   1230 batches | lr 0.00025 | ms/batch 507.80 | loss  3.16 | bpc   4.56462
| epoch   1 step     1240 |   1240 batches | lr 0.00025 | ms/batch 507.20 | loss  3.19 | bpc   4.60011
| epoch   1 step     1250 |   1250 batches | lr 0.00025 | ms/batch 506.59 | loss  3.51 | bpc   5.06809
| epoch   1 step     1260 |   1260 batches | lr 0.00025 | ms/batch 507.52 | loss  3.54 | bpc   5.10440
| epoch   1 step     1270 |   1270 batches | lr 0.00025 | ms/batch 508.00 | loss  3.75 | bpc   5.40666
| epoch   1 step     1280 |   1280 batches | lr 0.00025 | ms/batch 507.94 | loss  3.41 | bpc   4.92323
| epoch   1 step     1290 |   1290 batches | lr 0.00025 | ms/batch 507.55 | loss  3.36 | bpc   4.84357
| epoch   1 step     1300 |   1300 batches | lr 0.00025 | ms/batch 507.95 | loss  3.49 | bpc   5.03097 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64, 3.17, 3.3, 3.16, 3.19, 3.51, 3.54, 3.75, 3.41, 3.36, 3.49]
| epoch   1 step     1310 |   1310 batches | lr 0.00025 | ms/batch 507.87 | loss  3.37 | bpc   4.85947
| epoch   1 step     1320 |   1320 batches | lr 0.00025 | ms/batch 508.12 | loss  3.51 | bpc   5.07019
| epoch   1 step     1330 |   1330 batches | lr 0.00025 | ms/batch 507.60 | loss  3.40 | bpc   4.91051
| epoch   1 step     1340 |   1340 batches | lr 0.00025 | ms/batch 508.83 | loss  3.44 | bpc   4.96150
| epoch   1 step     1350 |   1350 batches | lr 0.00025 | ms/batch 508.75 | loss  3.49 | bpc   5.03741
| epoch   1 step     1360 |   1360 batches | lr 0.00025 | ms/batch 509.60 | loss  3.48 | bpc   5.01755
| epoch   1 step     1370 |   1370 batches | lr 0.00025 | ms/batch 511.17 | loss  3.47 | bpc   5.00395
| epoch   1 step     1380 |   1380 batches | lr 0.00025 | ms/batch 510.38 | loss  3.26 | bpc   4.70894
| epoch   1 step     1390 |   1390 batches | lr 0.00025 | ms/batch 511.50 | loss  3.74 | bpc   5.38962
| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 511.18 | loss  3.70 | bpc   5.34349 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64, 3.17, 3.3, 3.16, 3.19, 3.51, 3.54, 3.75, 3.41, 3.36, 3.49, 3.37, 3.51, 3.4, 3.44, 3.49, 3.48, 3.47, 3.26, 3.74, 3.7]
| epoch   1 step     1410 |   1410 batches | lr 0.00025 | ms/batch 510.02 | loss  3.58 | bpc   5.16241
| epoch   1 step     1420 |   1420 batches | lr 0.00025 | ms/batch 509.51 | loss  3.35 | bpc   4.83026
| epoch   1 step     1430 |   1430 batches | lr 0.00025 | ms/batch 508.83 | loss  3.42 | bpc   4.93705
| epoch   1 step     1440 |   1440 batches | lr 0.00025 | ms/batch 508.74 | loss  3.57 | bpc   5.15613
| epoch   1 step     1450 |   1450 batches | lr 0.00025 | ms/batch 507.97 | loss  3.34 | bpc   4.81538
| epoch   1 step     1460 |   1460 batches | lr 0.00025 | ms/batch 509.06 | loss  3.33 | bpc   4.80408
| epoch   1 step     1470 |   1470 batches | lr 0.00025 | ms/batch 508.80 | loss  3.39 | bpc   4.88957
| epoch   1 step     1480 |   1480 batches | lr 0.00025 | ms/batch 507.57 | loss  3.28 | bpc   4.72798
| epoch   1 step     1490 |   1490 batches | lr 0.00025 | ms/batch 509.07 | loss  3.52 | bpc   5.07534
| epoch   1 step     1500 |   1500 batches | lr 0.00025 | ms/batch 509.23 | loss  4.05 | bpc   5.84347 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64, 3.17, 3.3, 3.16, 3.19, 3.51, 3.54, 3.75, 3.41, 3.36, 3.49, 3.37, 3.51, 3.4, 3.44, 3.49, 3.48, 3.47, 3.26, 3.74, 3.7, 3.58, 3.35, 3.42, 3.57, 3.34, 3.33, 3.39, 3.28, 3.52, 4.05]
| epoch   1 step     1510 |   1510 batches | lr 0.00025 | ms/batch 508.79 | loss  3.62 | bpc   5.22922
| epoch   1 step     1520 |   1520 batches | lr 0.00025 | ms/batch 508.07 | loss  3.43 | bpc   4.95136
| epoch   1 step     1530 |   1530 batches | lr 0.00025 | ms/batch 508.33 | loss  3.32 | bpc   4.79452
| epoch   1 step     1540 |   1540 batches | lr 0.00025 | ms/batch 508.23 | loss  3.41 | bpc   4.91808
| epoch   1 step     1550 |   1550 batches | lr 0.00025 | ms/batch 508.99 | loss  3.45 | bpc   4.97304
| epoch   1 step     1560 |   1560 batches | lr 0.00025 | ms/batch 508.77 | loss  3.37 | bpc   4.86780
| epoch   1 step     1570 |   1570 batches | lr 0.00025 | ms/batch 508.66 | loss  3.48 | bpc   5.01553
| epoch   1 step     1580 |   1580 batches | lr 0.00025 | ms/batch 509.22 | loss  3.51 | bpc   5.05774
| epoch   1 step     1590 |   1590 batches | lr 0.00025 | ms/batch 509.22 | loss  3.70 | bpc   5.33874
| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 511.56 | loss  3.85 | bpc   5.55015 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64, 3.17, 3.3, 3.16, 3.19, 3.51, 3.54, 3.75, 3.41, 3.36, 3.49, 3.37, 3.51, 3.4, 3.44, 3.49, 3.48, 3.47, 3.26, 3.74, 3.7, 3.58, 3.35, 3.42, 3.57, 3.34, 3.33, 3.39, 3.28, 3.52, 4.05, 3.62, 3.43, 3.32, 3.41, 3.45, 3.37, 3.48, 3.51, 3.7, 3.85]
| epoch   1 step     1610 |   1610 batches | lr 0.00025 | ms/batch 497.48 | loss  3.62 | bpc   5.22341
| epoch   1 step     1620 |   1620 batches | lr 0.00025 | ms/batch 501.16 | loss  3.64 | bpc   5.25605
| epoch   1 step     1630 |   1630 batches | lr 0.00025 | ms/batch 504.36 | loss  3.64 | bpc   5.25477
| epoch   1 step     1640 |   1640 batches | lr 0.00025 | ms/batch 503.79 | loss  3.43 | bpc   4.94783
| epoch   1 step     1650 |   1650 batches | lr 0.00025 | ms/batch 503.97 | loss  3.60 | bpc   5.19036
| epoch   1 step     1660 |   1660 batches | lr 0.00025 | ms/batch 503.65 | loss  3.44 | bpc   4.96001
| epoch   1 step     1670 |   1670 batches | lr 0.00025 | ms/batch 504.48 | loss  3.73 | bpc   5.37936
| epoch   1 step     1680 |   1680 batches | lr 0.00025 | ms/batch 504.80 | loss  3.65 | bpc   5.26414
| epoch   1 step     1690 |   1690 batches | lr 0.00025 | ms/batch 504.42 | loss  3.78 | bpc   5.45344
| epoch   1 step     1700 |   1700 batches | lr 0.00025 | ms/batch 504.19 | loss  3.53 | bpc   5.09379 | current losses [4.41, 3.48, 3.31, 3.33, 3.37, 3.5, 3.42, 3.52, 3.43, 3.34, 3.53, 3.53, 3.74, 3.54, 3.34, 3.37, 3.22, 3.47, 3.22, 3.24, 3.6, 3.61, 3.57, 3.61, 3.23, 3.57, 3.33, 3.66, 3.41, 3.37, 3.37, 3.44, 3.56, 3.45, 3.61, 3.35, 3.64, 3.71, 4.04, 3.55, 3.37, 3.46, 3.73, 3.34, 3.57, 3.5, 3.28, 3.62, 3.3, 3.21, 3.27, 3.26, 3.32, 3.33, 3.68, 3.36, 3.45, 3.67, 3.72, 3.41, 3.17, 3.29, 3.24, 3.41, 3.43, 3.68, 3.79, 3.67, 3.48, 3.64, 3.73, 3.47, 3.56, 3.36, 3.18, 3.42, 3.42, 3.69, 3.68, 3.31, 3.31, 3.5, 3.44, 3.37, 3.46, 3.41, 3.52, 3.44, 3.51, 3.46, 3.63, 3.35, 3.26, 3.52, 3.32, 3.71, 3.32, 3.29, 3.35, 3.41, 3.3, 3.51, 3.32, 3.35, 3.3, 3.36, 3.36, 3.52, 3.54, 3.33, 3.25, 3.3, 3.51, 3.3, 3.15, 3.21, 3.3, 3.42, 3.23, 3.64, 3.17, 3.3, 3.16, 3.19, 3.51, 3.54, 3.75, 3.41, 3.36, 3.49, 3.37, 3.51, 3.4, 3.44, 3.49, 3.48, 3.47, 3.26, 3.74, 3.7, 3.58, 3.35, 3.42, 3.57, 3.34, 3.33, 3.39, 3.28, 3.52, 4.05, 3.62, 3.43, 3.32, 3.41, 3.45, 3.37, 3.48, 3.51, 3.7, 3.85, 3.62, 3.64, 3.64, 3.43, 3.6, 3.44, 3.73, 3.65, 3.78, 3.53]
| epoch   1 step     1710 |   1710 batches | lr 0.00025 | ms/batch 504.21 | loss  3.57 | bpc   5.14974
| epoch   1 step     1720 |   1720 batches | lr 0.00025 | ms/batch 504.58 | loss  3.39 | bpc   4.88402
| epoch   1 step     1730 |   1730 batches | lr 0.00025 | ms/batch 503.92 | loss  3.23 | bpc   4.66447
| epoch   1 step     1740 |   1740 batches | lr 0.00025 | ms/batch 504.21 | loss  3.33 | bpc   4.80029
| epoch   1 step     1750 |   1750 batches | lr 0.00025 | ms/batch 504.32 | loss  3.41 | bpc   4.91775
----------------------------------------------------------------------------------------------------
Exiting from training early
