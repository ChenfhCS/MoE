====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-040754
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 505.48 | loss  4.31 | bpc   6.21408
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 509.88 | loss  3.48 | bpc   5.01983
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 508.56 | loss  3.30 | bpc   4.76260
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 507.19 | loss  3.33 | bpc   4.79789
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 509.51 | loss  3.37 | bpc   4.86758
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 507.85 | loss  3.50 | bpc   5.04766
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 509.86 | loss  3.42 | bpc   4.93496
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 513.35 | loss  3.52 | bpc   5.08122
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 510.16 | loss  3.42 | bpc   4.94077
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 510.25 | loss  3.35 | bpc   4.82633 | current losses [4.31, 3.48, 3.3, 3.33, 3.37, 3.5, 3.42, 3.52, 3.42, 3.35]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 513.46 | loss  3.53 | bpc   5.09365
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 512.60 | loss  3.53 | bpc   5.09154
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 511.74 | loss  3.74 | bpc   5.38927
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 509.87 | loss  3.54 | bpc   5.11423
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 508.76 | loss  3.35 | bpc   4.82590
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 510.55 | loss  3.37 | bpc   4.86261
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 510.44 | loss  3.22 | bpc   4.64643
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 512.39 | loss  3.47 | bpc   5.00675
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 508.91 | loss  3.22 | bpc   4.64940
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 512.43 | loss  3.24 | bpc   4.67109 | current losses [4.31, 3.48, 3.3, 3.33, 3.37, 3.5, 3.42, 3.52, 3.42, 3.35, 3.53, 3.53, 3.74, 3.54, 3.35, 3.37, 3.22, 3.47, 3.22, 3.24]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 510.57 | loss  3.45 | bpc   4.97331
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 511.35 | loss  3.62 | bpc   5.21696
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 513.59 | loss  3.57 | bpc   5.15626
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 509.12 | loss  3.63 | bpc   5.24158
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 510.08 | loss  3.25 | bpc   4.68859
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 512.31 | loss  3.56 | bpc   5.13213
----------------------------------------------------------------------------------------------------
Exiting from training early
