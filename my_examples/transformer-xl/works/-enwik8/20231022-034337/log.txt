====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 12
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 2
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - expert_parallel : True
    - log_interval : 10
    - eval_interval : 4000
    - work_dir : works/-enwik8/20231022-034337
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - moe : True
    - moe_num_expert : 4
    - moe_top_k : 2
    - local_rank : 0
    - fuse_token : True
    - tied : True
    - n_token : 204
    - n_all_param : 116669692
    - n_nonemb_param : 116564016
====================================================================================================
#params = 116669692
#non emb params = 116564016
| epoch   1 step       10 |     10 batches | lr 0.00025 | ms/batch 492.78 | loss  4.13 | bpc   5.95451
| epoch   1 step       20 |     20 batches | lr 0.00025 | ms/batch 507.40 | loss  3.24 | bpc   4.68115
| epoch   1 step       30 |     30 batches | lr 0.00025 | ms/batch 508.33 | loss  2.82 | bpc   4.07195
| epoch   1 step       40 |     40 batches | lr 0.00025 | ms/batch 508.68 | loss  2.76 | bpc   3.97921
| epoch   1 step       50 |     50 batches | lr 0.00025 | ms/batch 509.16 | loss  2.77 | bpc   3.99489
| epoch   1 step       60 |     60 batches | lr 0.00025 | ms/batch 510.19 | loss  2.90 | bpc   4.17968
| epoch   1 step       70 |     70 batches | lr 0.00025 | ms/batch 510.47 | loss  2.82 | bpc   4.07327
| epoch   1 step       80 |     80 batches | lr 0.00025 | ms/batch 511.44 | loss  2.78 | bpc   4.00987
| epoch   1 step       90 |     90 batches | lr 0.00025 | ms/batch 511.54 | loss  2.76 | bpc   3.97967
| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 512.50 | loss  2.68 | bpc   3.86616 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68]
| epoch   1 step      110 |    110 batches | lr 0.00025 | ms/batch 513.99 | loss  3.57 | bpc   5.14382
| epoch   1 step      120 |    120 batches | lr 0.00025 | ms/batch 517.76 | loss  3.34 | bpc   4.81385
| epoch   1 step      130 |    130 batches | lr 0.00025 | ms/batch 519.81 | loss  3.32 | bpc   4.78708
| epoch   1 step      140 |    140 batches | lr 0.00025 | ms/batch 520.23 | loss  3.06 | bpc   4.41148
| epoch   1 step      150 |    150 batches | lr 0.00025 | ms/batch 520.37 | loss  2.83 | bpc   4.08856
| epoch   1 step      160 |    160 batches | lr 0.00025 | ms/batch 520.88 | loss  2.82 | bpc   4.06571
| epoch   1 step      170 |    170 batches | lr 0.00025 | ms/batch 522.96 | loss  2.61 | bpc   3.75977
| epoch   1 step      180 |    180 batches | lr 0.00025 | ms/batch 521.39 | loss  2.82 | bpc   4.06951
| epoch   1 step      190 |    190 batches | lr 0.00025 | ms/batch 521.69 | loss  2.54 | bpc   3.66631
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 521.89 | loss  2.50 | bpc   3.60395 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5]
| epoch   1 step      210 |    210 batches | lr 0.00025 | ms/batch 522.07 | loss  2.70 | bpc   3.89394
| epoch   1 step      220 |    220 batches | lr 0.00025 | ms/batch 522.32 | loss  2.95 | bpc   4.25610
| epoch   1 step      230 |    230 batches | lr 0.00025 | ms/batch 522.93 | loss  2.78 | bpc   4.01090
| epoch   1 step      240 |    240 batches | lr 0.00025 | ms/batch 522.37 | loss  2.59 | bpc   3.73155
| epoch   1 step      250 |    250 batches | lr 0.00025 | ms/batch 522.87 | loss  2.51 | bpc   3.62237
| epoch   1 step      260 |    260 batches | lr 0.00025 | ms/batch 522.50 | loss  2.69 | bpc   3.88760
| epoch   1 step      270 |    270 batches | lr 0.00025 | ms/batch 523.00 | loss  2.49 | bpc   3.58985
| epoch   1 step      280 |    280 batches | lr 0.00025 | ms/batch 524.72 | loss  2.59 | bpc   3.73685
| epoch   1 step      290 |    290 batches | lr 0.00025 | ms/batch 524.55 | loss  2.06 | bpc   2.97357
| epoch   1 step      300 |    300 batches | lr 0.00025 | ms/batch 523.90 | loss  1.83 | bpc   2.63771 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83]
| epoch   1 step      310 |    310 batches | lr 0.00025 | ms/batch 524.92 | loss  1.72 | bpc   2.48733
| epoch   1 step      320 |    320 batches | lr 0.00025 | ms/batch 524.46 | loss  1.94 | bpc   2.79759
| epoch   1 step      330 |    330 batches | lr 0.00025 | ms/batch 524.34 | loss  2.47 | bpc   3.56298
| epoch   1 step      340 |    340 batches | lr 0.00025 | ms/batch 524.35 | loss  2.28 | bpc   3.28888
| epoch   1 step      350 |    350 batches | lr 0.00025 | ms/batch 525.13 | loss  2.28 | bpc   3.28768
| epoch   1 step      360 |    360 batches | lr 0.00025 | ms/batch 524.36 | loss  2.30 | bpc   3.31953
| epoch   1 step      370 |    370 batches | lr 0.00025 | ms/batch 525.21 | loss  2.47 | bpc   3.56932
| epoch   1 step      380 |    380 batches | lr 0.00025 | ms/batch 524.25 | loss  2.43 | bpc   3.49954
| epoch   1 step      390 |    390 batches | lr 0.00025 | ms/batch 524.39 | loss  2.55 | bpc   3.68589
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 524.79 | loss  2.41 | bpc   3.47035 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41]
| epoch   1 step      410 |    410 batches | lr 0.00025 | ms/batch 525.91 | loss  2.28 | bpc   3.28771
| epoch   1 step      420 |    420 batches | lr 0.00025 | ms/batch 524.12 | loss  2.21 | bpc   3.18756
| epoch   1 step      430 |    430 batches | lr 0.00025 | ms/batch 523.94 | loss  2.45 | bpc   3.53090
| epoch   1 step      440 |    440 batches | lr 0.00025 | ms/batch 524.33 | loss  2.21 | bpc   3.18842
| epoch   1 step      450 |    450 batches | lr 0.00025 | ms/batch 523.93 | loss  2.31 | bpc   3.33868
| epoch   1 step      460 |    460 batches | lr 0.00025 | ms/batch 523.95 | loss  2.34 | bpc   3.37410
| epoch   1 step      470 |    470 batches | lr 0.00025 | ms/batch 523.86 | loss  2.22 | bpc   3.19763
| epoch   1 step      480 |    480 batches | lr 0.00025 | ms/batch 523.72 | loss  2.37 | bpc   3.41267
| epoch   1 step      490 |    490 batches | lr 0.00025 | ms/batch 523.76 | loss  2.18 | bpc   3.13973
| epoch   1 step      500 |    500 batches | lr 0.00025 | ms/batch 523.66 | loss  2.12 | bpc   3.06523 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12]
| epoch   1 step      510 |    510 batches | lr 0.00025 | ms/batch 523.40 | loss  2.17 | bpc   3.13731
| epoch   1 step      520 |    520 batches | lr 0.00025 | ms/batch 524.18 | loss  2.17 | bpc   3.12893
| epoch   1 step      530 |    530 batches | lr 0.00025 | ms/batch 524.84 | loss  2.11 | bpc   3.04546
| epoch   1 step      540 |    540 batches | lr 0.00025 | ms/batch 524.34 | loss  2.13 | bpc   3.07570
| epoch   1 step      550 |    550 batches | lr 0.00025 | ms/batch 524.37 | loss  2.23 | bpc   3.21317
| epoch   1 step      560 |    560 batches | lr 0.00025 | ms/batch 524.04 | loss  2.08 | bpc   3.00547
| epoch   1 step      570 |    570 batches | lr 0.00025 | ms/batch 524.81 | loss  2.15 | bpc   3.10722
| epoch   1 step      580 |    580 batches | lr 0.00025 | ms/batch 524.00 | loss  2.23 | bpc   3.21182
| epoch   1 step      590 |    590 batches | lr 0.00025 | ms/batch 523.94 | loss  2.16 | bpc   3.10968
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 524.06 | loss  2.16 | bpc   3.11039 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16]
| epoch   1 step      610 |    610 batches | lr 0.00025 | ms/batch 523.74 | loss  2.02 | bpc   2.91818
| epoch   1 step      620 |    620 batches | lr 0.00025 | ms/batch 524.08 | loss  2.00 | bpc   2.88246
| epoch   1 step      630 |    630 batches | lr 0.00025 | ms/batch 524.38 | loss  2.01 | bpc   2.90106
| epoch   1 step      640 |    640 batches | lr 0.00025 | ms/batch 524.59 | loss  2.06 | bpc   2.97277
| epoch   1 step      650 |    650 batches | lr 0.00025 | ms/batch 524.61 | loss  2.13 | bpc   3.07452
| epoch   1 step      660 |    660 batches | lr 0.00025 | ms/batch 526.15 | loss  2.25 | bpc   3.24748
| epoch   1 step      670 |    670 batches | lr 0.00025 | ms/batch 524.65 | loss  2.16 | bpc   3.12132
| epoch   1 step      680 |    680 batches | lr 0.00025 | ms/batch 524.30 | loss  1.89 | bpc   2.72440
| epoch   1 step      690 |    690 batches | lr 0.00025 | ms/batch 523.94 | loss  1.66 | bpc   2.39438
| epoch   1 step      700 |    700 batches | lr 0.00025 | ms/batch 524.78 | loss  2.22 | bpc   3.20249 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22]
| epoch   1 step      710 |    710 batches | lr 0.00025 | ms/batch 524.50 | loss  1.83 | bpc   2.63568
| epoch   1 step      720 |    720 batches | lr 0.00025 | ms/batch 524.34 | loss  2.03 | bpc   2.92317
| epoch   1 step      730 |    730 batches | lr 0.00025 | ms/batch 524.33 | loss  2.11 | bpc   3.04226
| epoch   1 step      740 |    740 batches | lr 0.00025 | ms/batch 523.93 | loss  1.94 | bpc   2.79490
| epoch   1 step      750 |    750 batches | lr 0.00025 | ms/batch 524.47 | loss  1.97 | bpc   2.84788
| epoch   1 step      760 |    760 batches | lr 0.00025 | ms/batch 524.37 | loss  1.90 | bpc   2.74430
| epoch   1 step      770 |    770 batches | lr 0.00025 | ms/batch 524.31 | loss  1.97 | bpc   2.84288
| epoch   1 step      780 |    780 batches | lr 0.00025 | ms/batch 524.36 | loss  2.09 | bpc   3.01531
| epoch   1 step      790 |    790 batches | lr 0.00025 | ms/batch 525.15 | loss  2.04 | bpc   2.94307
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 524.76 | loss  1.98 | bpc   2.86192 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98]
| epoch   1 step      810 |    810 batches | lr 0.00025 | ms/batch 524.77 | loss  1.97 | bpc   2.84796
| epoch   1 step      820 |    820 batches | lr 0.00025 | ms/batch 524.35 | loss  2.01 | bpc   2.89718
| epoch   1 step      830 |    830 batches | lr 0.00025 | ms/batch 525.01 | loss  1.84 | bpc   2.65450
| epoch   1 step      840 |    840 batches | lr 0.00025 | ms/batch 524.79 | loss  1.99 | bpc   2.87296
| epoch   1 step      850 |    850 batches | lr 0.00025 | ms/batch 524.74 | loss  2.01 | bpc   2.90116
| epoch   1 step      860 |    860 batches | lr 0.00025 | ms/batch 524.21 | loss  1.94 | bpc   2.79355
| epoch   1 step      870 |    870 batches | lr 0.00025 | ms/batch 523.98 | loss  1.87 | bpc   2.69715
| epoch   1 step      880 |    880 batches | lr 0.00025 | ms/batch 524.74 | loss  1.87 | bpc   2.70443
| epoch   1 step      890 |    890 batches | lr 0.00025 | ms/batch 523.53 | loss  1.76 | bpc   2.53770
| epoch   1 step      900 |    900 batches | lr 0.00025 | ms/batch 525.16 | loss  2.11 | bpc   3.03965 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11]
| epoch   1 step      910 |    910 batches | lr 0.00025 | ms/batch 524.23 | loss  1.78 | bpc   2.57148
| epoch   1 step      920 |    920 batches | lr 0.00025 | ms/batch 523.85 | loss  1.92 | bpc   2.77679
| epoch   1 step      930 |    930 batches | lr 0.00025 | ms/batch 523.72 | loss  1.83 | bpc   2.63897
| epoch   1 step      940 |    940 batches | lr 0.00025 | ms/batch 523.25 | loss  1.79 | bpc   2.58717
| epoch   1 step      950 |    950 batches | lr 0.00025 | ms/batch 524.12 | loss  1.88 | bpc   2.71036
| epoch   1 step      960 |    960 batches | lr 0.00025 | ms/batch 523.87 | loss  2.09 | bpc   3.00813
| epoch   1 step      970 |    970 batches | lr 0.00025 | ms/batch 523.98 | loss  1.91 | bpc   2.75751
| epoch   1 step      980 |    980 batches | lr 0.00025 | ms/batch 524.02 | loss  1.78 | bpc   2.56115
| epoch   1 step      990 |    990 batches | lr 0.00025 | ms/batch 524.11 | loss  1.88 | bpc   2.70652
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 524.37 | loss  1.82 | bpc   2.62765 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82]
| epoch   1 step     1010 |   1010 batches | lr 0.00025 | ms/batch 524.74 | loss  1.72 | bpc   2.47651
| epoch   1 step     1020 |   1020 batches | lr 0.00025 | ms/batch 524.73 | loss  1.88 | bpc   2.71928
| epoch   1 step     1030 |   1030 batches | lr 0.00025 | ms/batch 524.93 | loss  1.79 | bpc   2.58872
| epoch   1 step     1040 |   1040 batches | lr 0.00025 | ms/batch 525.02 | loss  1.74 | bpc   2.50975
| epoch   1 step     1050 |   1050 batches | lr 0.00025 | ms/batch 524.58 | loss  1.84 | bpc   2.65193
| epoch   1 step     1060 |   1060 batches | lr 0.00025 | ms/batch 524.99 | loss  2.03 | bpc   2.93344
| epoch   1 step     1070 |   1070 batches | lr 0.00025 | ms/batch 523.87 | loss  1.86 | bpc   2.68314
| epoch   1 step     1080 |   1080 batches | lr 0.00025 | ms/batch 524.50 | loss  1.74 | bpc   2.50705
| epoch   1 step     1090 |   1090 batches | lr 0.00025 | ms/batch 524.29 | loss  1.69 | bpc   2.43923
| epoch   1 step     1100 |   1100 batches | lr 0.00025 | ms/batch 524.00 | loss  1.80 | bpc   2.59340 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8]
| epoch   1 step     1110 |   1110 batches | lr 0.00025 | ms/batch 523.87 | loss  1.71 | bpc   2.47295
| epoch   1 step     1120 |   1120 batches | lr 0.00025 | ms/batch 524.14 | loss  1.77 | bpc   2.55691
| epoch   1 step     1130 |   1130 batches | lr 0.00025 | ms/batch 525.08 | loss  1.94 | bpc   2.79842
| epoch   1 step     1140 |   1140 batches | lr 0.00025 | ms/batch 525.87 | loss  1.73 | bpc   2.50230
| epoch   1 step     1150 |   1150 batches | lr 0.00025 | ms/batch 524.35 | loss  1.74 | bpc   2.50742
| epoch   1 step     1160 |   1160 batches | lr 0.00025 | ms/batch 524.44 | loss  1.70 | bpc   2.44716
| epoch   1 step     1170 |   1170 batches | lr 0.00025 | ms/batch 524.37 | loss  1.79 | bpc   2.57685
| epoch   1 step     1180 |   1180 batches | lr 0.00025 | ms/batch 523.98 | loss  1.76 | bpc   2.54254
| epoch   1 step     1190 |   1190 batches | lr 0.00025 | ms/batch 524.62 | loss  1.76 | bpc   2.53979
| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 524.10 | loss  1.88 | bpc   2.70635 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8, 1.71, 1.77, 1.94, 1.73, 1.74, 1.7, 1.79, 1.76, 1.76, 1.88]
| epoch   1 step     1210 |   1210 batches | lr 0.00025 | ms/batch 523.82 | loss  1.71 | bpc   2.47216
| epoch   1 step     1220 |   1220 batches | lr 0.00025 | ms/batch 524.22 | loss  1.74 | bpc   2.51094
| epoch   1 step     1230 |   1230 batches | lr 0.00025 | ms/batch 523.82 | loss  1.77 | bpc   2.56009
| epoch   1 step     1240 |   1240 batches | lr 0.00025 | ms/batch 525.26 | loss  1.62 | bpc   2.34342
| epoch   1 step     1250 |   1250 batches | lr 0.00025 | ms/batch 524.17 | loss  1.86 | bpc   2.68181
| epoch   1 step     1260 |   1260 batches | lr 0.00025 | ms/batch 524.16 | loss  1.82 | bpc   2.62738
| epoch   1 step     1270 |   1270 batches | lr 0.00025 | ms/batch 524.03 | loss  1.93 | bpc   2.78388
| epoch   1 step     1280 |   1280 batches | lr 0.00025 | ms/batch 524.38 | loss  1.75 | bpc   2.51808
| epoch   1 step     1290 |   1290 batches | lr 0.00025 | ms/batch 523.94 | loss  1.80 | bpc   2.59937
| epoch   1 step     1300 |   1300 batches | lr 0.00025 | ms/batch 524.46 | loss  1.85 | bpc   2.67078 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8, 1.71, 1.77, 1.94, 1.73, 1.74, 1.7, 1.79, 1.76, 1.76, 1.88, 1.71, 1.74, 1.77, 1.62, 1.86, 1.82, 1.93, 1.75, 1.8, 1.85]
| epoch   1 step     1310 |   1310 batches | lr 0.00025 | ms/batch 524.70 | loss  1.82 | bpc   2.62820
| epoch   1 step     1320 |   1320 batches | lr 0.00025 | ms/batch 524.42 | loss  1.81 | bpc   2.60731
| epoch   1 step     1330 |   1330 batches | lr 0.00025 | ms/batch 524.67 | loss  1.78 | bpc   2.56373
| epoch   1 step     1340 |   1340 batches | lr 0.00025 | ms/batch 524.46 | loss  1.86 | bpc   2.67900
| epoch   1 step     1350 |   1350 batches | lr 0.00025 | ms/batch 523.86 | loss  1.88 | bpc   2.71767
| epoch   1 step     1360 |   1360 batches | lr 0.00025 | ms/batch 524.41 | loss  1.90 | bpc   2.74084
| epoch   1 step     1370 |   1370 batches | lr 0.00025 | ms/batch 524.40 | loss  1.79 | bpc   2.58073
| epoch   1 step     1380 |   1380 batches | lr 0.00025 | ms/batch 526.41 | loss  1.67 | bpc   2.40734
| epoch   1 step     1390 |   1390 batches | lr 0.00025 | ms/batch 524.58 | loss  1.63 | bpc   2.34472
| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 523.91 | loss  1.62 | bpc   2.33894 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8, 1.71, 1.77, 1.94, 1.73, 1.74, 1.7, 1.79, 1.76, 1.76, 1.88, 1.71, 1.74, 1.77, 1.62, 1.86, 1.82, 1.93, 1.75, 1.8, 1.85, 1.82, 1.81, 1.78, 1.86, 1.88, 1.9, 1.79, 1.67, 1.63, 1.62]
| epoch   1 step     1410 |   1410 batches | lr 0.00025 | ms/batch 524.08 | loss  1.93 | bpc   2.78816
| epoch   1 step     1420 |   1420 batches | lr 0.00025 | ms/batch 523.57 | loss  1.89 | bpc   2.72374
| epoch   1 step     1430 |   1430 batches | lr 0.00025 | ms/batch 523.86 | loss  1.82 | bpc   2.62650
| epoch   1 step     1440 |   1440 batches | lr 0.00025 | ms/batch 523.49 | loss  2.05 | bpc   2.95782
| epoch   1 step     1450 |   1450 batches | lr 0.00025 | ms/batch 524.10 | loss  1.79 | bpc   2.58789
| epoch   1 step     1460 |   1460 batches | lr 0.00025 | ms/batch 523.95 | loss  1.82 | bpc   2.62123
| epoch   1 step     1470 |   1470 batches | lr 0.00025 | ms/batch 524.09 | loss  1.81 | bpc   2.61720
| epoch   1 step     1480 |   1480 batches | lr 0.00025 | ms/batch 523.76 | loss  1.74 | bpc   2.50770
| epoch   1 step     1490 |   1490 batches | lr 0.00025 | ms/batch 525.35 | loss  1.83 | bpc   2.64497
| epoch   1 step     1500 |   1500 batches | lr 0.00025 | ms/batch 524.01 | loss  1.93 | bpc   2.78370 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8, 1.71, 1.77, 1.94, 1.73, 1.74, 1.7, 1.79, 1.76, 1.76, 1.88, 1.71, 1.74, 1.77, 1.62, 1.86, 1.82, 1.93, 1.75, 1.8, 1.85, 1.82, 1.81, 1.78, 1.86, 1.88, 1.9, 1.79, 1.67, 1.63, 1.62, 1.93, 1.89, 1.82, 2.05, 1.79, 1.82, 1.81, 1.74, 1.83, 1.93]
| epoch   1 step     1510 |   1510 batches | lr 0.00025 | ms/batch 524.29 | loss  1.79 | bpc   2.57784
| epoch   1 step     1520 |   1520 batches | lr 0.00025 | ms/batch 524.52 | loss  1.86 | bpc   2.68810
| epoch   1 step     1530 |   1530 batches | lr 0.00025 | ms/batch 524.43 | loss  1.72 | bpc   2.48067
| epoch   1 step     1540 |   1540 batches | lr 0.00025 | ms/batch 525.43 | loss  1.71 | bpc   2.46207
| epoch   1 step     1550 |   1550 batches | lr 0.00025 | ms/batch 525.04 | loss  1.92 | bpc   2.77257
| epoch   1 step     1560 |   1560 batches | lr 0.00025 | ms/batch 524.71 | loss  1.79 | bpc   2.58187
| epoch   1 step     1570 |   1570 batches | lr 0.00025 | ms/batch 525.11 | loss  1.67 | bpc   2.41359
| epoch   1 step     1580 |   1580 batches | lr 0.00025 | ms/batch 524.68 | loss  1.84 | bpc   2.65843
| epoch   1 step     1590 |   1590 batches | lr 0.00025 | ms/batch 524.35 | loss  1.94 | bpc   2.79225
| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 524.13 | loss  1.91 | bpc   2.75363 | current losses [4.13, 3.24, 2.82, 2.76, 2.77, 2.9, 2.82, 2.78, 2.76, 2.68, 3.57, 3.34, 3.32, 3.06, 2.83, 2.82, 2.61, 2.82, 2.54, 2.5, 2.7, 2.95, 2.78, 2.59, 2.51, 2.69, 2.49, 2.59, 2.06, 1.83, 1.72, 1.94, 2.47, 2.28, 2.28, 2.3, 2.47, 2.43, 2.55, 2.41, 2.28, 2.21, 2.45, 2.21, 2.31, 2.34, 2.22, 2.37, 2.18, 2.12, 2.17, 2.17, 2.11, 2.13, 2.23, 2.08, 2.15, 2.23, 2.16, 2.16, 2.02, 2.0, 2.01, 2.06, 2.13, 2.25, 2.16, 1.89, 1.66, 2.22, 1.83, 2.03, 2.11, 1.94, 1.97, 1.9, 1.97, 2.09, 2.04, 1.98, 1.97, 2.01, 1.84, 1.99, 2.01, 1.94, 1.87, 1.87, 1.76, 2.11, 1.78, 1.92, 1.83, 1.79, 1.88, 2.09, 1.91, 1.78, 1.88, 1.82, 1.72, 1.88, 1.79, 1.74, 1.84, 2.03, 1.86, 1.74, 1.69, 1.8, 1.71, 1.77, 1.94, 1.73, 1.74, 1.7, 1.79, 1.76, 1.76, 1.88, 1.71, 1.74, 1.77, 1.62, 1.86, 1.82, 1.93, 1.75, 1.8, 1.85, 1.82, 1.81, 1.78, 1.86, 1.88, 1.9, 1.79, 1.67, 1.63, 1.62, 1.93, 1.89, 1.82, 2.05, 1.79, 1.82, 1.81, 1.74, 1.83, 1.93, 1.79, 1.86, 1.72, 1.71, 1.92, 1.79, 1.67, 1.84, 1.94, 1.91]
----------------------------------------------------------------------------------------------------
Exiting from training early
